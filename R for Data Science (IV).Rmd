---
title: "R for Data Science (IV)"
author: "Koji Mizumura"
date: "2018年7月25日"
output: slidy_presentation
---
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    df_print: "paged"
    
ioslides_presentation
  
# IV Model  

# Chapter 22. Introduction  


Now that you are equippted with powerful programming tools, we can finally return to modeling. You will use your new tools of data wrangling and programming, to fit many models and understand how they work.

The focus of this book is on exploration, not confimation or formal inference. But you will leanr a few basic tools to help you understand the variation within your models. 

The gola of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). Here we only cover “predictive” models, which, as the name suggests, generate predictions. There is another type of model that we’re not going to discuss: “data discovery” models. These models don’t make predictions, but instead help you discover interesting relationships within your data. (These two categories of models are sometimes called supervised and unsupervised, but I don’t think that terminology is particularly illuminating.)

This book is not going to give you a deep understanding of the mathematical theory that underlies models. It will, however, build your intuition about how statistical models work, and give you a family of useful tools that allow you to use models to better understand your data:

- In [model basics](http://r4ds.had.co.nz/model-basics.html), you’ll learn how models work mechanistically, focussing on the important family of linear models. You’ll learn general tools for gaining insight into what a predictive model tells you about your data, focussing on simple simulated datasets.

- In [model building](http://r4ds.had.co.nz/model-building.html), you’ll learn how to use models to pull out known patterns in real data. Once you have recognised an important pattern it’s useful to make it explicit in a model, because then you can more easily see the subtler signals that remain.

- In [many models](http://r4ds.had.co.nz/many-models.html), you’ll learn how to use many simple models to help understand complex datasets. This is a powerful technique, but to access it you’ll need to combine modeling and progamming tools.

These topics are notable because of what they don't include: any tools for quantitatively assessing models. This is deliberate : precisely quantifying a model requires a couple of big ideas that we just don't have the space to cover here. For now, you'll rely on qualitative assessment and your natural speticism. In [Learning more about models](http://r4ds.had.co.nz/model-building.html#learning-more-about-models), we'll point you to other resources where you can learn more.

## 22.1 Hypothesis generation vs. Hypothesis confirmation

In this book, we are going to use models as a tool for exploration, completing the trifecta of the tools for EDA that were introduced in Part 1. his is not how models are usually taught, but as you will see, models are an important tool for exploration. Traditionally, the focus of modelling is on inference, or for confirming that an hypothesis is true. Doing this correctly is not complicated, but it is hard. There is a pair of ideas that you must understand in order to do inference correctly:

1. Each observation can either be used for exploration or confimration, not both.
2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.

This is necessary because to confirm a hypothesis you must use data independent of the data you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but yo ushould neve sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading. 

If you are serious about doing an confirmation analysis, one approach is to split your data into three pieces before you begin the analysis:

1. 60% of your data goes into a **training** (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.

2. 20% goes into a **query** set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.

3. 20% is held back for a **test** set. You can only use this data ONCE, to test your final model.

---

# Chapter 23: Model basics
## 23.1 Introduction

The goal of a model is to provide a simple low-dimensional summary of a dataset. In the context of this book we’re going to use models to partition data into patterns and residuals. Strong patterns will hide subtler trends, so we’ll use models to help peel back layers of structure as we explore a dataset.

However, before we can start using models on interestin, real, datasets, you need to understand the basics of how model works. For that reason, this chapter of the book is unique because it uses only simulated datasets. These datasets are very simple, and not at all interesting, but they will help you understand the essence of modelling before you apply the same techniques to real data in the next chapter.

There are two parts to a model:
1.First, you define a **family of models** that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like $y = a_1 * x + a_2 or y = a_1 * x ^ a_2$. Here, x and y are known variables from your data, and $a_1$ and $a_2$ are parameters that can vary to capture different patterns.

2. Next, you generate a **fitted model** by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like $y = 3 * x + 7 or y = 9 * x ^ 2$.

It's important to understand that a fitter model is just the closest model from  afamily of models. That implies that you have the "best" model (according to some criteria); it does not imply that you have a good model and it certainly doesn't imply that the model is "true". George Box puts this well in his famous aphrism:

> All models are wrong, but some are useful

It's worth reading the fuller context of the quote:

> Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

> For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?”.

The goal of a model is not to uncover truth, but to discovera simple approaximation that is still useful.

### 23.1.1. Pre-requisites
In this chapter, we will use the `modelr` package which wraps around base R's modeling functions to make them work naturally in a pipe.

<YAML header>
- echo: show/hide
- eval: evaluate/not code
- include: evaluate, but include/not include

```{r eval=FALSE}
library(tidyverse)
library(modelr)
options(na.action=na.warn)
```

## 23.2 A simpler model
Lets take a look at the simulated dataset `sim1`, included with the `modelr` package. It contains two continuous variables, `x`, `y`. Let's plot them to see how they're related:
```{r simple model ggplot}
library(ggplot2)
library(modelr)
library(tidyverse)

ggplot(sim1,aes(x,y))+
  geom_point()+theme_minimal()
```

You can see a strong pattern in the data. Let's use a model to capture that pattern and make it explicit. It's our job to supply the basic form of the model. In this case, the relationship looks linear, i.e., $y=a_0+a_1*x$. Let's start by getting a fell for what models from that family look like by randomly generating a few and overlaying them on the data. 

For this simple case, we can use `geom_abline()` which takes a slope and intercept as parameters. Later on we’ll learn more general techniques that work with any model.
```{r simple model with geom_abline}
models <- tibble(
  a1=runif(250,-20,40),
  a2=runif(250,-5,5)
)
# runif(250,min=-20,max=40)

modelsX <- tibble(
  a1=runif(1,-20,40),
  a2=runif(1,-5,5)
) %>% print()

head(models)

ggplot(data=sim1,aes(x,y))+
  geom_abline(aes(intercept=a1,slope=a2),data=models,alpha=1/4)+
  geom_point()
```

There are 250 models on the above plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is “close” to the data. We need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of a_0 and a_1 that generate the model with the smallest distance from this data.

One easy place to start is to find the vertical distance between each point and the model, as in the following diagram. (Note that I’ve shifted the x values slightly so you can see the individual distances.)

This distance is just the difference between the y value given by the model (the **prediction**), and the actual y value in the data (the **response**).

To compute this distance, we first turn our model family into an R function. This takes the model parameters and the data as inputs, and gives values predicted by the model as output:
```{r simple model}
model1 <- function(a,data){
  a[1]+data$x*a[2]
}
model1(c(7,1,5),sim1)
```

Next, we need some way to compute an overall distance between the predicted and actual values. In other words, the plot above shows 30 distances: how do we collapse that into a single number?

One common way to do this in statistics to use the *“root-mean-squared deviation”*. We compute the difference between actual and predicted, square them, average them, and the take the square root. This distance has lots of appealing mathematical properties, which we’re not going to talk about here. You’ll just have to take my word for it!
```{r}
measure_distance <- function(mod,data){
  diff <- data$y - model1(mod,data)
  sqrt(mean(diff^2))
}
measure_distance(c(7,1.5),sim1)
```

Now we can use `purrr` to compute the distance for all the models defined above. We need a helper function because our distance function expects the model as a numeric vector of length 2.
```{r}
sim1_dist <- function(a1,a2){
  measure_distance(c(a1,a2),sim1)
}

models <- models %>% 
  mutate(dist=purrr::map2_dbl(a1,a2,sim1_dist))
models
```

Next, let's overlay the 10 best models on the data. I've coloured the models by `-dist`: this is an easy way to make sure that the best models (i.e., the ones with the smallest distance) get the highest colours.
```{r}
ggplot(sim1,aes(x,y))+
  geom_point(size=2,colour="grey30")+
  geom_abline(
    aes(intercept=a1,slope=a2,colour=-dist),
    data=filter(models,rank(dist)<=10)
  )
```

We can also think about these models as observations, and visualising with a scatterplot of `a1` vs `a2`, again coloured by `-dist`. We can no longer directly see how the model compares to the data, but we can see many models at once. Again, I’ve highlighted the 10 best models, this time by drawing red circles underneath them.

```{r}
ggplot(models,aes(a1,a2))+
  geom_point(data=filter(models,rank(dist)<=10),size=4,colour="red")+
  geom_point(aes(colour=-dist))
```

Instead of trying lots of random models, we could be more systematic and generate an evenly spaced grid of points (this is called a grid search). I picked the parameters of the grid roughly by looking at where the best models were in the plot above.
```{r}
grid <- expand.grid(
  a1=seq(-5,20,length=25),
  a2=seq(1,3,length=25)
) %>% 
  mutate(dist=purrr::map2_dbl(a1,a2,sim1_dist))

grid %>% 
  ggplot(aes(a1,a2))+
  geom_point(data=filter(grid,rank(dist)<=10),size=4,colour="red")+
  geom_point(aes(colour=-dist))
```

When you overlay the best 10 models back on the original data, they all look pretty good:
```{r}
ggplot(sim1,aes(x,y))+
  geom_point(size=2,colour="grey30")+
  geom_abline(
    aes(intercept = a1, slope = a2, colour=-dist),
    data=filter(grid,rank(dist)<=10)
  )
```

You could imagine iteratively making the grid finer and finer until you narrowed in on the best model. But there’s a better way to tackle that problem: a numerical minimisation tool called `Newton-Raphson search`. The intuition of Newton-Raphson is pretty simple: you pick a starting point and look around for the steepest slope. You then ski down that slope a little way, and then repeat again and again, until you can’t go any lower. In R, we can do that with `optim()`:
```{r optim}
best <- optim(c(0,0),measure_distance,data=sim1)
best$par
#> [1] 4.22 2.05

# ggplot(sim1,aes(x,y))+
#   geom_point(size=2,colour="light grey")+
#   geom_abline(intercept=c(3,2),slope=c(2,3),colour="blue")

ggplot(sim1,aes(x,y))+
  geom_point(size=2,colour="grey30")+
  geom_abline(intercept=best$par[1],slope=best$par[2])
```

Don’t worry too much about the details of how `optim()` works. It’s the intuition that’s important here. If you have a function that defines the distance between a model and a dataset, an algorithm that can minimise that distance by modifying the parameters of the model, you can find the best model. The neat thing about this approach is that it will work for any family of models that you can write an equation for.








