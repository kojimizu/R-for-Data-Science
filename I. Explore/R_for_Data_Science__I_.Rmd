---
output:
  word_document: default
  html_document: default
---

# R for Data Science
http://r4ds.had.co.nz/exploratory-data-analysis.html

# Chapter1: Data Visualization with gglopt2
## First steps

mpg dataset:
- model: model name
- displ: engine displacement, in litres
- year: year of manufacture
- cyl: number of cylinders
- trans: type of transmission
- drv: f = front-wheel drive, r = rear wheel drive, 4 = 4wd
- cty: city miles per gallon
- hwy: highway miles per gallon
- fl: fuel type
- class: "type" of car

```{r}
# install.packages("dplyr")
# install.packages("tidyverse")
library(tidyverse)
head(mpg)
dim(mpg)
#mpg - #drv: front-wheel drive, r:rear wheel drive, 4: 4wd

ggplot(data=mpg)+
  geom_point(mapping=aes(x=displ,y=hwy))

ggplot(data=mpg)+
  geom_point(mapping=aes(x=hwy,y=cyl))
plot(mpg$hwy,mpg$cyl)

# geom_point
ggplot(data=mpg)+
  geom_point(mapping=aes(x=class,y=drv))
```

## Aesthetic mappings 
We can add a third variable, like class, to a two-dimensioal scatterplot by mapping it to an aesthetic.
```{r}
library(ggplot2)
# color
ggplot(data=mpg)+
  geom_point(mapping=aes(x=displ,y=hwy,color=class))
# size
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,size=class))
# alpha - transparency
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,shape=class))
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,alpha=class))
```

## Exercises 
```{r ggplot exercise}
#1.ggplot visualization
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy),color='blue')

#2. which variables in mpg are categorical? Which variables are continuous
head(mpg)
# categoirical: model,manufacturer,trans,drv,fl,class
# 

#3. Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,color=cty))
#ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,shape=cty))
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy,size=cty))

#4 What happens if you map the same variable to multiple aesthetics?
ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = cty, size = cty),alpha=0.5,bins=0.5)

#5 What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)
#Answer: Stroke controls the width of the border of certain shapes. Those shapes which have borders are the only ones that stroke can alter.

#6 What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ < 5)?
ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = displ < 5))

#ggplot turns displ < 5 into a boolean (or dummy) variable on the fly and maps that T or F to the colour argument.

```

## Facets
One way to add additional variables is with aesthetics. Another way, particularty useful for categorical variables, is to split the plot into facets, subplots that each display one subset of the data.
```{r facet_wrap, facet_grid}
library(ggplot2)

# facet wrap
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_wrap(~class,nrow=2)

# facet_grid: combination of two variables
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_grid(drv~cyl)
```

## Facet Exercise
1. What happens if you facet on a continuous variable?
- We can still plot it anyway.

```{r facet on continuous variable}
head(mpg)
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_wrap(~cty)
```

2. What do the emplty cells in a lot with facet_grid(drv~cyl) mean? How do they relate to this plot?
- There are combinations where there are no data points.
```{r facet_grid}
ggplot(data=mpg)+geom_point(mapping=aes(x=drv,y=cyl))+facet_grid(drv~cyl)
```

3. What plots does the following code make? What does.do?
- The dot controls whether the facetting will be done row or column wise. For example,`facet_grid(. ~ drv)` will sue drv as rows while `facet_grid(.~drv)` will use it as columns. `facet_grid(~drv)` will do the same as the columns wise facetting but `facet_grid(drv~)` won't because a formula object needs to have something after the ~. 


```{r dot in facet_grid}
# drv~ row divide
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_grid(drv~.)
# column divide by cyl
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_grid(.~cyl)
```

4 . Take the first faceted plot in this section.
```{r }
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))+facet_wrap(~class,nrow=2)
```

What are the advantages of using faceting instead of the colour aesthetic? What are the disadvantages?
- facetting is better when we want to pay attention to particular facets alone while using the color aesthetic is better for within group patterss.

5. Read ?facet_warp. What does nrow do? What does ncol do? What other options control the layout of the individual panels? why doesn't facet_grid have nrow and ncolu variables?
- nrow controls the number of rows for the total number of facets whereas ncol controls the number of columns. Other options can control interesting parameters. For example, scales can control whether each plot has its own y axis with scales = "free", as in allow the axes to be free. The function also has the labeller option to change the names of each facet and other options like strip.position for the position of the facets labels. Read ?facet_wrap for more options.

## Geometric Objects

Geom is the geometrical object that a plot uses to represent data. To change the geom in the plot, change the geom function that we add to ggplot.
```{r geometric objects}
# left
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))

# right
ggplot(data=mpg)+geom_smooth(mapping=aes(x=displ,y=hwy))
```

Every geom function in ggplot2 takes a mapping argument. However, not every aesthetic works with every geom. 
- geom_smooth() will draw a different line, with a different linetype, for each unique value of the variable that we map to linetype.
```{r geom_smooth with different linetype}
ggplot(data=mpg)+geom_smooth(mapping=aes(x=displ,y=hwy,linetype=drv))
```

ggplot provides over 30 geoms, and extension packages provide even more (refer: http://www.ggplot2-exts.org)

Many geoms like geom_smoooth(), use a single geompetric object to displya multiple rows of data. For these geoms, we can see the group aesthetic to a categorical variable to draw multiple objects.
```{r group aesthetic}
# standard
ggplot(data=mpg)+geom_smooth(mapping=aes(x=displ,y=hwy))
# group aesthetic
ggplot(data=mpg)+geom_smooth(mapping=aes(x=displ,y=hwy,group=drv))
# color aesthetic
ggplot(data=mpg)+geom_smooth(mapping=aes(x=displ,y=hwy,color=drv))
```

To display multiple geoms in the same plot, add multiple geom functions to ggplot()
```{r multiple geoms}
ggplot(data=mpg, mapping=aes(x=displ,y=hwy))+geom_point(aes(color=class))+geom_smooth()
```

We can use the same idea to specify different data for each layer.Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only.

```{r }
ggplot(data=mpg,mapping=aes(x=displ,y=hwy))+
  geom_point(mapping=aes(color=class))+
  geom_smooth(data=filter(mpg,class=='subcompact'),se=FALSE)

library(dplyr)
distinct(mpg)
```

## geometric objects exercise
1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart?
```{r geom exercise 1}
library(ggplot2)
data(package="ggplot2")

# line chart
library(magrittr)
head(mpg)
mpg %>% group_by(year) %>%
  mutate(m=mean(cty)) %>% ggplot(aes(year,m)) +
  geom_line()

# summarise() function
# mpg %>% group_by(year)%>% summarise(m=mean(cty)) %>% ggplot(aes(year,m))+geom_line()

# Boxplot
ggplot(mpg,aes(class,hwy))+geom_boxplot()

# Histogram
ggplot(mpg,aes(displ))+geom_histogram(bins=60)

# Area chart
huron <- data.frame(year=1875:1972, level=as.vector(LakeHuron))
ggplot(huron,aes(year,level))+geom_area()
```

2. Run this code in your head, and precdict what the output will look like. Then, run the code in R and check the predictions:
```{r geom exercise 2}
ggplot(mpg,mapping=aes(x=displ,y=hwy,color=drv))+
  geom_point()+geom_smooth(se=FALSE)
```

3. What does sho.legend=FALSE do? What happens if we remove it? Why do we think we used it earlier in this chapter?
`It removes the legend. It gives a cleaner plot when its clear that the grouping is done on a specific variable`

4. What does the se argument to geom_smooth() do?
`It removes the confidence interval from the smooothed line`

5. Will these two graphs look different? Why?
```{r geom exercise 5}
ggplot(mpg,aes(displ,hwy))+geom_point()+geom_smooth()

ggplot() + 
  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))
```

6. Let's recreate the R code necessary to generate the following graphs.
```{r geom exercise 6}
# 1st.
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth(se = F)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth(aes(group = drv), se = F)

# 2nd.
ggplot(mpg, aes(displ, hwy, colour = drv)) +
  geom_smooth(se = F) +
  geom_point()

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = drv)) +
  geom_smooth(se = F)

# 3rd.
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = drv)) +
  geom_smooth(aes(linetype = drv), se = F)

# You can do this one by choosing a shape which has a border and simply colour
# the border with `colour` and the insides with `fill` (which is matched to drv).
# Then make the whole point a bit bigger with size
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(fill = drv), shape = 21, stroke = 2, colour = "white", size = 3)
```

## Statistical Transformation
Next, let's take a look at a bar chart. 

We can generally use geoms and stas interchangeably. For example, we can re-create the previous plot using stat_count() instead of geom_bar(): 
```{r statistical transformation}
library(ggplot2)
ggplot(diamonds)+stat_count(mapping=aes(x=cut))
```

Every geom has a default stat, and every stat has a default geom. This means we can typically use geoms without worrying about the underlying statistical transformation. 

### geom_bar stat="identity"
We want to override the default stat. In the following code, we change the stat of geom_bar from count to indentiy. This lets us map the height of the bars to the ra value of y variable.
```{r}
demo <- tribble(
  ~a, ~b,
  "bar_1", 20,
  "bar_2", 30,
  "bar_3", 40)

ggplot(data=demo)+
  geom_bar(aes(x=a,y=b),stat="identity")
```

### transformed variables to aesthetic
We can override the deafault mapping from transformed variables to aesthetics. For example, we want to display a bar chart of proportion, rather than count:
```{r}
ggplot(diamonds)+
  geom_bar(aes(x=cut,y=..prop..,group=1))
```

### statistical transformation
We wnt to draw greater attention to the statistical transformation in the code. we use stat_summary, which summarizes the y values for each unique x value, to draw attention to the summary that we are computing.
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
```

ggplot2 provides over 20 stats for you to use. Each stat is a function, so we can get help in the usual way, e.g., ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.

### Statistical transformation exercise
1. What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
```

`stat_summary` is associated with `geom_pointrange`.
```{r}
ggplot(diamonds)+
  geom_pointrange(aes(cut,depth,ymin=depth,ymax=depth))
```

2. What does geom_col() do? How is it different to geom_bar()?
`geom_col` leaves the data as it is. `geom_bar()` creates two variables (count and prod) and then graphs the count data on the y axis. With `geom_col` we can plot the values of any x variable against any y value.
```{r}
# For example, plotting exactly x to y values.
aggregate.data.frame(diamonds$price,list(diamonds$cut),mean,na.rm=T) %>%
  print(.)%>%
  ggplot(aes(Group.1,x))+
  geom_col()
```

3. Most geoms and stats come in pars that are almost always used in concert. Read through the documentation and make a list of all the pairs what do they have in common?

4. What variables does stat_smoooth() compute? What parameters control its hehaviour?

5. Inour proportion bar chart, we need to set group=1. Why? In other words, what is the problem with these two graphs?

- `stat_smoooth` computes the y, the predicted value of y for each x value. Also, it computes the se of that value predicted, together with the upper and lower bound of that point prediction. It can compute different methods such as `lm`,`glm`,`lowess` among others. See method in `?stat_smooth`. The statistic can be controlled with the method argument. 

We can see the values by wrapping any plot that has geom_smoooth() with ggplot_build(). 

In our porportion bar chart, we need to set group=1. Why? In other words, what is the problem with these two graphs? 

Ctl + Shift + M: %>% pipe operator

```{r}
# Each cut is treated as a searapte group that sums to 1.
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut, y = ..prop..))

# If you calculate it manually, it doesn't matter
m <- ggplot(data = diamonds)
m + geom_bar(aes(cut, ..count../sum(..count..)))

diamonds %>%
  count(cut) %>%
  mutate(prop = n/sum(n)) %>%
  ggplot(aes(cut, prop)) + geom_bar(stat = "identity") # or geom_col()

ggplot(diamonds, aes(cut)) + geom_bar(aes(y = ..count../sum(..count..)))

# By specifying group = 1, you treat all cut groups as 1 group.
ggplot(diamonds, aes(cut)) + geom_bar(aes(y = ..prop.., group = 1))
# and thus all the proportions are done calculate as a single group

```

## Position Adjustments
There's one more piece of magic associated with bar charts. We can color a bar chart uing either the color aesthetic, or more usefully, fill:
```{r Position adjustment - color/fill}
ggplot(data=diamonds)+geom_bar(mapping=aes(x=cut,color=cut))
ggplot(data=diamonds)+geom_bar(mapping=aes(x=cut,fill=cut))
```

Note what will happen if we map the fill aesthetic to another variable, like clarity.
```{r}
ggplot(data=diamonds)+geom_bar(mapping=aes(x=cut,fill=clarity))
```

The stacking is performed automatically bt the position adjustments specificed by the position argument. If we don't want a stacked bar chart, we can use one of three other options: "identity","dodge","fill".
- position="identity": will place each object exactly where it falls in the context of the group
```{r}
library(ggplot2)
library(magrittr)
diamonds %>% ggplot(aes(x=cut,fill=clarity)) +geom_bar(alpha=1/5,position="identity")

diamonds %>% ggplot(aes(x=cut,color=clarity))+geom_bar(fill=NA,position="identity")
```

- position="fill": works like stacking but makes each set of stacked bars the same height.
```{r}
ggplot(data=diamonds)+geom_bar(aes(x=cut,fill=clarity),position = "fill")
```

- position="dodge": places overlapping objects directly beside one another. This make it easier to compare individual values:
```{r}
diamonds %>% ggplot()+geom_bar(aes(x=cut,fill=clarity),position="dodge")
```

Overplotting: We can avoid overplotting gridding by setting the position adjustment to "jitter". 
- position="jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noises. 
```{r}
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy),position="jitter")
```

## Position adjustment exercise
1. What is the problem with this plot? How could we improve it?
```{r}
ggplot(mpg,mapping=aes(x=cty,y=hwy))+geom_point(position="jitter")

# OR 
mpg %>% ggplot(aes(x=cty,y=hwy))+geom_point(alpha=0.2)+geom_jitter()
```

2. What parameters to geom_jitter() control te amount of jittering?


# Chapter 2: Workflow Basics 
## Workflow Exercise
1. Why does this code not work?
```{r}
my_variable <- 10
my_variable
```

2. Tweak each of the following R commands so that they run correctly.
```{r}
# mpg dataset
# install.packages("tidyverse")
library(tidyverse)
ggplot(data=mpg)+geom_point(mapping=aes(x=displ,y=hwy))
filter(mpg,cyl==8)

# diamonds dataset
library(ggplot2)
filter(diamonds,carat>3)
```

3. Press Alt-Shift-K. What happens? How can you get to the same place using the menues?
Knit function works.

# Chapter 3. Data Transformation with dplyr
## Introduction

### Prerequisites
```{r Data transformation - prerequisites}
# install.packages("nycflights13")
library(nycflights13)
library(tidyverse)
```

nycflights13
This data frame contains all 336,776 flights that departed from NYC in 2013. The data comes from the US Bureau of Transportation Statistics(https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0).

```{r}
head(flights)
```

This data frame prints differently because it's a tibble. Tibbles are data frames, but slightly tweaked to work better in the tidyverse. 

Rows of three letter abbreviations under the columns names, which describe type of each variable.
- int: integers
- dbl: doubles (real numbers)
- chr: character vectors (strings)
- dttm: date-times (data + time)

There are other three common types of variables that aren't used in this dataset, but we will encounter in the book.
- lgl: logical vectors (T or F)
- fctr: factors (categorical variables with fixed possible values)
- date: dates

### dplyr Basics
five key dplyr functions that allow us to solve vast majority of the data manipulation challenges:
1. pick observations by their values (filter())
2. Reoder the rows (arrange)
3. Pick variables by their names (select())
4. Create new variables with functions of existing variables (mutate())
5. Collapse many variable down to a single summary (summarize())

These can all be used in conjunction with group_by(), which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. 

All verbs work similarly:
1. The first argument is a data frame.
2. The subsequent arguments describe what to do with the data frame, using the variable names (without quotes)
3. The result is a new data frame

## 5.2 Filter Rows with filter()
filter() allows us to subset observations based on their values.
```{r data transformation - filter()}
head(flights)

library(magrittr)
flights %>% filter(month==1,day==1) %>% head(10)
```

### Comparison 
To use filtering effectively, we have to know how to select the observations that we want using the comparison operators. R provides the standard suite: >, >=, <, <=, !=(not equal), and ==(equal).
```{r data transformation - comparison}
sqrt(2)^2==2
1/49 *49 ==1

# compute use finite precision arithmetic so remmeber that every number calculated is an approximation. Instead of relpying on ==, use near().
near(sqrt(2)^2,2)
near(1/49*49,1)
```

### Logical Operators
Multiple arguments to filter() are combined with "and": every expression must be true in order for a row to be included in the output. 
```{r filter - logical operators}
library(nycflights13)
flights %>% filter(month==11 | month==12) %>% head()
```

A useful shorthand for this problem is `x %in% y`. This will select every row where x is one of the values in y. We could use it to rewrite the preceding code: 
```{r}
nov_dec <- filter(flights,month %in% c(11,12))
head(nov_dec)
```

Sometimes we can simplify complicated subsetting by remembering De Morgan's low - !(x&y) is the same as !x |!y, and !(x|y) is the same as !x & !y. 
```{r}
flights %>% filter(arr_delay>120 |dep_delay>120) %>% head()
flights %>% filter(arr_delay<=120,dep_delay<=120) %>% head()
```

### Missing values 

One important feature of R that can make comparison tricky is missing values or NAs. NA represents an unknown values so missing values are "contagious", almost any operation involving an unknown value will also be unknown.
```{r}
NA >5
10 == NA
NA + 10
NA /2

# The most confusing result is this one:
NA==NA

# It's easiest to understand why this is true with a bit more context
x <- NA
y <- NA
x == y

# To determine if a value is missing
is.na(x)

# filter() only includes rows where the condition is true; it excludes both FALSE and NA values. If we want to preserve missing values, ask for them explicitly:
df <- tibble(x=c(1,NA,3))
filter(df,x>1)
filter(df,is.na(x)|x>1)
```

### Exercises
1. Find all flights that:
a. had an arrival delay of two or more hours
b. flew to Houston (IAH or HOU)
c. were operated by United, American or Delta
d. Departed in summer (July, August and September)
e. Arrived more than two hours late, but didn't leave late
f. Were delayed by at least an hour, but made up over 30minutes in flight
g. Departed between midngight and 6 a.m.

```{r filter() exercise}
library(nycflights13)
head(flights)
# a. arrival delay
flights %>% filter(arr_delay>=120) %>% head()
# b. flew to Houston
flights %>% filter(dest %in% c("IAH","HOU")) %>% head()
# c. operated comp
flights %>% filter(carrier %in% c("AA","DL","UA")) %>% head()
# d. seasonality
flights %>% filter(month %in% 7:9) %>% head()
# e. delay in arrival time
flights %>% filter(arr_delay >120,dep_delay<=0) %>% head()
# f. delay in arrival time
# if dep delay is 10 minutes late then arr_delay should be 10 mins lates. 10-10=0, so air time was on time. 
flights %>%  filter(dep_delay>=60,(dep_delay-arr_delay>30)) %>% head()
# g. departed between midnight and 6 am
flights %>%  filter(dep_time>=2400 | dep_time<=0600) %>% head()
```

2. Another usefuldplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges?
```{r between()}
filter(flights,between(dep_time,601,2359)) %>% head()
```

3. How many flights have a missing de_time? What other variables are missing? What might these row reporesent?
```{r filter - NA values}
# missmap package
# install.packages("Amelia")
library(Amelia)
missmap(flights)

# missing value calculation
sum(is.na(flights$dep_time))
map_dbl(flights,~sum(is.na(.x)))
```

4. Why is NA^0 not missing? Why is NA|TRUE not missing? Why is FALSE & NA not missing? Can you figure out the general rule? 
Because anything is `^0` equals `1`. Because NA|TRUE is saying whether one of the two is TRUE and the second one is. Because at least one of the two expressions can be tested: FALSE&NA.In NA&NA neither can be tested and the results is `NA&NA`. 

The general rule is that whenever there is a logical expressions, if one can be tested, then the results shouldn't be `NA`. And any operation that the results is determined, regardless of the number, the inputting `NA` does not affect the result.

## 5.2 Arrange Rows with arrange()
arrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. 

If we provide more than one column name, each additional column will be used to break ties in the values of preceding columns:
```{r arrange()}
flights %>% arrange(year,month,day) %>% head(10)
```

Use desc() to reorder by a column in descending order:
```{r arrange() - desc}
flights %>% arrange(desc(arr_delay)) %>% head()
```

Missing values are always sorted at the end:
```{r}
df <- tibble(x=c(5,2,NA))
arrange(df,x)
arrange(df,desc(x))
```

### Exercise
1. How could you use arrange() to sort all missing values to the start?
```{r}
# arrange exercise 1 ----
library(tidyverse)
df <- tibble(x=c(5,2,NA),
             y=c(2,NA,2))
rowSums(df)
arrange(df)
arrange(df,desc(is.na(x)))
```

We're basically saying, those which are `TRUE` to being `NA`, sort othem in descending order. 

2. Sort flights to find the most delayed flights. Find the flights that left earliest. 
```{r}
# arrange exercise 2 ----
library(nycflights13)
head(flights)
arrange(flights,dep_delay) %>% head()
arrange(flights,desc(dep_delay)) %>% head()
```

3. Sort flights to find the fastest flights.
```{r}
# arrange exercise 3  ----
arrange(flights,air_time) %>% head()
```

4. Which flights traveled the longest? Which traveled the shortest?
```{r}
# arrange() exercise 4 ----
# shortest
flights %>% 
  arrange(air_time) %>% 
  select(carrier,flight,air_time)
# fastest
flights %>% 
  arrange(-air_time) %>% 
  select(carrier,flight,air_time)
```

## 5.4 Select Columns with select()

It's not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables we're actually intersted in. 

select() allows us to rapidly zoon in on a useful subset using operations based on the names of the variables. 
```{r}
# select columns by name
select(flights,year,month,day)
# select all columns between year and day
select(flights,year:day)
# select all columns except those from year to day 
select(flights,-(year:day))
```

There are number of helper functions we can use within select():
- starts_with("abc"):matches names that begin with "abc"
- ends_with("xyz"): matches names that end with "xyz"
- contains("ijk"): matches names that end with "xyz"
- matches("(.)\\1"):selects variables that match a regular expression. This only matches any variable that contain repeated characters. 
- num_range("x",1:3): matches x1, x2 and x3.

select() can be used to rename variables, but it's rearely useful because it drops all of the variable not explitly mentioned. Instead, rename(), which is a variant of select() that keeps all the variables that aren't explicitly mentioned. 

```{r}
# select() - rename function ----
rename(flights,tail_num=tailnum) %>% head()
select(flights, time_hour,air_time,everything())
```

### Exercises
1. Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.
```{r}
# select() - exercise
vars <- c('dep_time','dep_delay','arr_time','arr_delay')
select(flights,vars)
select(flights,matches('^dep|^arr'))

```

2. What happens if we include the name of a variable muptile times in a select call?
2
```{r}
select(flights,dep_time,dep_time)
# nothing, it just returns it one
```

3. What does the one_of() function do? Why might it be helpful in conjunction with this vector? 
- It works because select only accept variable names without `""` quotes. By including inside `one_of`, one can use character names. 
```{r}
vars <- c("year","month","day","dep_delay","arr_delay")
select(flights,one_of(vars))
```

4. Does the result of running the following code surprise us? How do the select helpers deal with case by default? How can you change that default?
```{r}
select(flights,contains("TIME",ignore.case=F))
```

## 5.5 Add New Variables with mutate()
Besides selecting sets of existing columns, it's often useful to add new columns that are functions of existing columns. mutate() always adds new columns at the end of the dataset, so we will start by creating a narrower dataset so we can see the new variables. 
```{r}
# mutate() --------
flights_sml <- select(flights,year:day,
                      ends_with('delay'),
                      distance,
                      air_time)
head(flights_sml)

# mutate function
mutate(flights_sml,
       gain=arr_delay-dep_delay,
       speed=distance/air_time*60)
```

Note that we can refer to columns that we have just created:
```{r}
# mutate() part2
mutate(flights_sml,
       gain=arr_delay-dep_delay,
       hours=air_time/60,
       gain_per_hour=gain/hours)
```

If we only want to keep the new variables, use transmute():
```{r}
transmute(flights,
          gain=arr_delay-dep_delay,
          hours=air_time/60,
          gain_per_hour=gain/hours)
```

### Useful Creation Functions
There are many functions for creating new variables that we can use with mutate(). The key property is that the function must be vectorized. It must take a vector of values as input, and returns a vector with the same number of values as output. 

1. Arithmetic operators +,-,*,/,^

2. Modular arithmetic %/%, %%
- %/% integer devision
- %% remainder, where 
x == y*(x%/%y)+(x%%y)
```{r}
transmute(flights, dep_time,
          hour=dep_time %/% 100,
          minute=dep_time %% 100)
```

3. Logs log(), log2(), log10()
4. offsets
- lead() and lag() allow us to refer to leading or lagging values. This allows us to compute running differences (x-lag(x)). They are most useful in conjunction with group_by().
```{r}
# lag(), lead() ---------
x <- 1:10
lag(x)
lead(x)
```

5. Cumulative and rolling aggregates
R provides functions fo running sums, products, mis and maxes:
cumsum(), cumprod(), cummin(), cummax(); and dplyer provides cummean() for cumulative means. 
```{r}
# cumsum, cummean --------
x
cumsum(x)
cummean(x)
```

6. Logicl comparisons <, <=, >, >=, !=
IF we are doing a complex sequence of logical operations, it's often a good idea to store the interim values in new variables so we can check that each step is working as expected.

7. Ranking
There are a number of ranking functions, but we should start with min_rank(). It does the most usual type of ranking, and the default gives the smallest values the smallest ranks.
```{r}
y <- c(1,2,2,NA,3,4)
min_rank(y)
min_rank(desc(y))
```

If min_rank() doesn't do what we need,look at the variants
- row_number()
- dense_rank()
- percent_rank()
- cume_dist()
- ntile()
```{r}
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
```

### mutate() exercise
1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they're not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. 
```{r}
head(flights)
mutate(flights, 
       dep_hour = dep_time %/% 100,
       de_minutes = dep_time %% 100)

hours2mins <- function(x){
  x%/% 100*60 + x%%100}

# with integer division
mutate(flights,
       dep_time=hours2mins(dep_time),
       sched_dep_time=hours2mins(sched_dep_time))

# with rounding operations
mutate(flights,
       dep_time=60*floor(dep_time/100)+(dep_time-floor(dep_time/100)*100),
       sched_dep_time=60*floor(sched_dep_time/100)+(sched_dep_time-floor(sched_dep_time/100)*100))
```

2. Compare air_time with arr_time-dep_time. What do you expect to see? What do you see? What do you need to do to fix it?
```{r}
flights %>% 
  mutate(dep_time = (dep_time %/% 100) * 60 + (dep_time %% 100),
         sched_dep_time = (sched_dep_time %/% 100) * 60 + (sched_dep_time %% 100),
         arr_time = (arr_time %/% 100) * 60 + (arr_time %% 100),
         sched_arr_time = (sched_arr_time %/% 100) * 60 + (sched_arr_time %% 100)) %>%
  transmute((arr_time - dep_time) %% (60*24) - air_time)
```

3. Compare dep_time, sched_dep_time, and dep_delay. How would we expect those three numbers to be related? 
```{r}
hours2mins <- function(x){
  x%/%100*60 + x%%100
}

# dep_time-shed_dep_time
select(flights,contains("dep")) %>% 
  mutate(dep_time_two=hours2mins(dep_time)-hours2mins(sched_dep_time))

## those two numbers don't match because we aren't accounting for flights
## where the departure time is the next dat from the scheduled departure time.

# 
select(flights,contains("dep")) %>% 
  mutate(dep_time_two=hours2mins(dep_time)-hours2mins(sched_dep_time)) %>% filter(dep_delay!=dep_time_two) %>% 
  mutate(dep_time_two=hours2mins(dep_time)-hours2mins(sched_dep_time-2400))
# there it is fixed
```

4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank()
```{r}
flights %>% 
  filter(min_rank(-(dep_delay)) %in% 1:10)

flights %>% 
  top_n(10,dep_delay)
```

5. What does 1:3 + 1:10 return? Why?
```{r}
x <- c(2,4,6,5,7,9,8,10,12,11)
p <- 1:3+1:10
p==x
```


## 5.6 Grouped Summaries with summarize()

The last key verb is summarize(). It collapses a data frame to a single row: 
```{r}
# summarize() -------
library(dplyr)
library(nycflights13)
summarize(flights,delay=mean(dep_delay,na.rm = T))
```

Summarize() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. 

Then, when we use the dplyr verbs on a grouped data frame, they will be automatically applied "by group". For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date. 
```{r}
by_day <- group_by(flights, year,month,day)
summarize(by_day,delay=mean(dep_delay,na.rm = T))
```

Together group_by() and summarize() provide one of the tools that you'll use most commonly when working with dplyr:grouped summaries. 

### Combining multiple operations with pipe

When we want to explore the relationship between the distance and average delay for each location. Using what we know about dployr we can write code like this with pipe operator
```{r}

library(magrittr)

by_dest <- flights %>% group_by(dest) 
delay <- summarize(by_dest,
                   count=n(),
                   dist=mean(distance,na.rm = T),
                   delay=mean(arr_delay,na.rm = T))
delay <- filter(delay,count>20,dest!="HNL")

# visualization
library(ggplot2)
ggplot(delay,mapping=aes(x=dist,y=delay))+
  geom_point(aes(size=count),
             alpha=0.3)+
  geom_smooth(se=F)
```

There are three steps to prepare this data:
1. Group flights by destination
2. Summarize to compute distance, average delay, and number of flights
3. Filter to remove noisy points aqnd Honolulu airport, which is almost twice as far aay as the next closest airport

This code is little frustrating to write because we have to give each intermediate data frame a name, even thought we don't care about it.

There is another way to To tackle this problem with the pipe %>%, 
```{r}
# pipe operator -----
delays <- flights %>% 
  group_by(dest) %>% 
  summarize(
    count=n(),
    dist=mean(distance,na.rm=T),
    delay=mean(arr_delay,na.rm=T)) %>% 
      filter(count>20,dest!="HNL")
head(delays)

```

### Missing values 
We may have wondered the na.rm rgument we used earier.
What happens if we don't set it.
-> we get a lot of missing values. 

The reason is that aggregation functions obey the usual rule of missing values: if there is any missing value in the input, the output will be a missing vlue as well. 

```{r}
# na.rm() argument chunk 1 -----
flights %>% 
  group_by(year,month,day) %>% 
  summarize(mean=mean(dep_delay))
```

Fortunately, all aggregation functions have an na.rm argument, which removes the missing values prior to computation.
```{r}
# na.rm() argument chunk 2 -----
flights %>% 
  group_by(year,month,day) %>% 
  summarize(mean=mean(dep_delay,na.rm=T))
```

In this case, where missing values represent cancelled flights, we could als tackle the problem by first removing the cancelled fligts. We'll save this dataset so we can reuse it in the next few examples:
```{r}
library(Amelia)
missmap(flights)
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay),!is.na(arr_delay))

missmap(not_cancelled)

not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarize(mean=mean(dep_delay))
```

### Counts
Whenever we do any agregation, its always a good idea to include either count(n()) or a count of nonmissing values, sum(!is.na(x)). 
```{r}
# counts() -----
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarize(
    delay=mean(arr_delay)
  )

ggplot(delays, aes(x=delay))+
  geom_freqpoly(binwidth=10)
```

We can get more inslights if we draw a scatterplot of number of flights versus average delay:
```{r}
# count() part 2 -----
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarize(
    delay=mean(arr_delay,na.rm=T),
    n=n()
  )
head(flights)
head(delays)
ggplot(delays,aes(x=n,y=delay))+
  geom_point(alpha=1/10)
```

Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: Whenever we plot a mean versus group size, we'll see that the variation decreases as the sample size increases. 
```{r}
# count() part 3 -----
delays %>% 
  filter(n>25) %>% 
  ggplot(mapping=aes(x=n,y=delay))+
  geom_point(alpha=1/10)
```

There is another common variation of this type of pattern. Here we use data from the Lahman package to compute the batting average(number of hits/number of attemps) of every major league baseball player.

When we plot the skill of the batter against the number of opportnuties to hit the ball, we see two patterns:
- As above, the variation in our aggregate decreases as we get more data points
- There is a positive correlation between skill and opportunities to hit the ball. This is because teams control who gets to play, and obviously they'll pick their best players:
```{r}
# count part 4 -----
# convert to a tible it prints nicely

# install.packages("Lahman")
library(Lahman)

batting <- 
  as_tibble(Lahman::Batting)

head(batting)

batter <- batting %>% 
  group_by(playerID) %>% 
  summarize(
    ba=sum(H,na.rm = TRUE)/sum(AB,na.rm = TRUE),
    ab=sum(AB,na.rm = TRUE)
  )

batter %>% 
  filter(ab>100) %>% 
  ggplot(aes(x=ab,y=ba))+geom_point(alpha=0.1)+geom_smooth(se=FALSE)
```

This also has important implications for ranking. If we naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled. 
```{r}
# count part 5 -----
batter %>% arrange(desc(ba)) %>% head(20)
```

### Useful Summary Functions
Just using means, counts, and sum can get usa long way, but R provides many other useful summary functions:

#### Measures of location
median (x)
```{r}
# useful summary functions 1 -----
head(not_cancelled)
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarize(
    # average delay:
    avg_delay1=mean(arr_delay),
    # average positive delay:
    avg_delay2=mean(arr_delay[arr_delay>0])
  )
```

#### Measures of spead sd(x), IQR(x), mad(x)
The mean squared deviations or standard deviations or sd is the standard measure of spread. 
```{r}
# useful summary functions 2 -----
not_cancelled %>% 
  group_by(dest) %>% 
  summarize(
    distance_sd=sd(distance)) %>% 
  arrange(desc(distance_sd))
```

#### Measures of rank  min(x), quantile(x,0.25),max(x)
Quantiles are genelization of the median. For example, quantile(x,0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%. 
```{r}
# useful summary functions 3 -----
not_cancelled %>% 
  group_by(year, month,day) %>% 
  summarize(
    first=min(dep_time),
    last=max(dep_time)
  )
```

#### Measure of position fitst(x), nth(x,2), last(x)
These work similarly to x[1],x[2],and x[length(x)], but let us set a default value if that position does not exist. For example, we can find the first and last departure for each day: 
```{r}
# useful summary functions 4 -----
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarize(first_dep=first(dep_time),
            last_dep=last(dep_time))
```

These functions are cmplementary to filtering on ranks.Filtering gives us all variables, with each observations in a separate row:
```{r}
# useful summary functions 5 -----
not_cancelled %>% 
  group_by(year,month,day) %>% 
  mutate(r=min_rank(desc(dep_time))) %>% 
  filter(r %in% range(r)) %>% 
  head(30)
```

#### Counts
We have seen n(), which takes no arguments, and returns the size of the current group. To count the number of non-missing values, use sum(!is.na(x)). To count the number of distance, use n_distinct(x):
```{r}
# counts 1 -----
# which destinations have the most carriers?
not_cancelled %>% 
  group_by(dest) %>% 
  summarize(carriers=n_distinct(carrier)) %>% 
  arrange(desc(carriers))
```

Counts are useful that dplyr provides a simple helper if all we want is a count:
```{r}
# counts 2 -----
not_cancelled %>% 
  count(dest)
```

We can optionally provide a weight variable. For example we could use this to "count(sum)" the total number of miles a plane floew:
```{r}
# counts 3 -----
not_cancelled %>% 
  count(tailnum,wt=distance) %>% 
  head()
```

#### counts and proportions of logical values 
sum(x>10), mean(y==0)
When used with numeric functions, TRUEs is convered to 1 and FALSe to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and mean(x) gives the propotion:
```{r}
# counts and porportions 1 -----
# How many flights left before 5am? (These usually indicate delayed flights from the previous day)
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarize(n_early=sum(dep_time<500)) %>% 
  head()

# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarize(hour_perc=mean(arr_delay>60))
```

### Grouping by Multiple Variables
When we group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset: 
```{r}
# Grouping with variables -----
daily <- flights %>% group_by(year,month,day)
head(per_day <- summarize(daily,flights=n()))
# number of flights per month
(per_month <- summarize(per_day,flights=sum(flights)))
# number of flights per year
(per_year <- summarize(per_month,flights=sum(flights)))
```

Be careful when progressively rolling up summaries: its OK for sums and counts, but we need to think about weighting means and variances, and its not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sum is the overvall sum, but the median groupwise medians is not the overvall median. 

### Ungrouping
If we need to remove grouping, and return to operations on ungrouped data, use ungroup():
```{r}
# ungrouping -----
daily %>% 
  ungroup() %>% 
  # no longer grouped by date
  summarize(flights=n())
  # all flights
```

### Exercise
1. Brainstorm at least five different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:
- A flights is 15 minutes early 50% of the time, and 15 minutes last 50% of the time
- A flight is always 10 minutes late
- A flights is 30 minutes early 50% of the time and 30 minutes late 50% of the time
- 99% of the time a flight is on time. 1% of the time its 2 hours late. Which is more improtant arrival delay or departure delay?
```{r}
# Exercise 1 -----
library(nycflights13)
head(flights)
df <- flights %>% filter(flight==1) 
df$arr_delay
mean(df$arr_delay,na.rm = T)

library(purrr)

# delay char
delay_char <- flights %>% 
  group_by(flight) %>% 
  summarize(n=n(),
            fifteen_early=mean(arr_delay==-15,na.rm=T),
            fifteen_late=mean(arr_delay==15,nar.rm=T),
            ten_always=mean(arr_delay==10,nar.rm=T),
            thirty_early=mean(arr_delay==-30,nar.rm=T),
            thirty_late=mean(arr_delay==30,nar.rm=T),
            percentage_on_time=mean(arr_delay==0,na.rm=T),
            twohours=mean(arr_delay>120,nar.rm=T)) %>% 
  map_if(is.double,round,2) %>% 
  as_tibble()
head(delay_char)
```

```{r}
# A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.
delay_char %>% filter(fifteen_early==0.5,fifteen_late==0.5)

# A flight is always 10 minutes late.
delay_char %>%  filter(ten_always==1)

# A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.
delay_char %>% filter(thirty_early==0.5,thirty_late==0.5)

# 99% of the time a flight is on time. 1% of the time it’s 2 hours late.
delay_char %>% 
  filter(percentage_on_time==0.99 & twohours ==0.01)
```

2. Come up with another approach that will give us the sample output as
```{r}
# Exercise 2 -----
# Come up with another approach that will give you the same output as:
library(Amelia)
missmap(flights)

# dateset refine
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay),!is.na(arr_delay))

not_cancelled %>% count(dest)
not_cancelled %>% count(tailnum,wt=distance)

#########

not_cancelled %>% 
  group_by(dest) %>% 
  summarize(n=n())

not_cancelled %>% 
  group_by(tailnum) %>% 
  tally(wt=distance)

not_cancelled %>% 
  group_by(tailnum) %>% 
  summarize(n=sum(distance))
```

3. Our definition of cancelled flights `(is.na(dep_delay) | is.na(arr_delay) )` is slightly suboptimal. Why? Which is the most important column?

Because if a flight didn't leave then it was cancelled. If the condition is.na(dep_delay) is met, then the flight was cancelled.

4. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?

- It looks like there is a positive relationship. The higher the average delay of the day, the higher the proportion of cancelled flights per day.

```{r}
# Exercise 4 -----
flights %>% 
  group_by(day) %>% 
  summarise(cancelled=mean(is.na(dep_delay)),
            mean_dep=mean(dep_delay,na.rm=T),
            mean_arr=mean(arr_delay,na.rm=T)) %>% 
  ggplot(aes(y=cancelled))+
  geom_point(aes(x=mean_dep),colour="red")+
  # geom_smooth(aes(x=mean_dep),colour="red")+
  geom_point(aes(x=mean_arr),colour="blue")+
  xlab("Avg delay per day")+ylab("Cancelled flights per day")
```

5. Which carrier has the worst delay? 
Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %>% group_by(carrier, dest) %>% summarise(n()))
```{r}
# Exercise 5 -----
flights %>% 
  group_by(carrier) %>% 
  summarise(dep_max=max(dep_delay,na.rm=T),
            delay_max=max(arr_delay,na.rm=T)) %>% 
  arrange(desc(dep_max,arr_max)) %>% 
  filter(1:n()==1)
```

6. For each plane, count the number of flights before the first delay of greater than 1 hour.
```{r}
# Exervise 6 -----
library(nycflights13)
flights %>% 
  mutate(dep_time=time_hour) %>% 
  group_by(tailnum) %>% 
  arrange(dep_time) %>% 
  mutate(cumulative = !cumany(arr_delay>60)) %>% 
  filter(cumulative==T)  %>% 
  tally(sort=TRUE) %>% 
  head()
```

7. What does the sort argument to count() do? When might we use it?
When we want to sort the cases based on the count
```{r}
# Exervise 7 -----
flights %>%
  count(flight, sort = T)
```

## Grouped Mutates (and Filter)
Grouping is most useful in conjunction with summarise(), buy we can also do convenience operations with mutate() and filter():
- Find the worst number of each group:
```{r}
# Grouped Mutate 1 -----
head(flights_sml)
flights_sml %>% 
  group_by(year,month,day) %>% 
  filter(rank(desc(arr_delay))<10) %>% 
  head()
```

- Find all groups bigger than a threshold:
```{r}
# Grouped Mutate 2 -----
popular_dests <- flights %>%
  group_by(dest) %>% 
  filter(n()>365)
head(popular_dests)
```

- Standardize to compute per group metrics:
```{r}
# Grouped Mutate 3 -----
library(tidyverse)
popular_dests %>% 
  filter(arr_delay>0) %>% 
  mutate(prop_delay=
           arr_delay/sum(arr_delay)) %>% 
  select(year:day,dest,arr_delay,prop_delay) %>% 
  head()
```

### Grouped Mutates - Exercises 
1. Refer back to the table of useful mutate and filtering functions. Describe how each operation changes when we combine it with grouping.

2. Which plane(tailnum) has the worst on-time record?
The flights listed are always late.
```{r Grouped Mutates - Exercises 1}
head(flights)

flights %>% 
  filter(!is.na(arr_delay)) %>% 
  group_by(tailnum) %>% 
  summarise(prop_time=sum(arr_delay<=30)/n(),
            mean_arr=mean(arr_delay,na.rm=T),
            fl=n()) %>% 
  arrange(desc(prop_time))
```

3. Wyat time of day should we fly if we want to avoid delays as much as possible?
```{r Grouped Mutates - Exercises 2}
flights %>% 
  group_by(hour) %>%
  filter(!is.na(dep_delay)) %>% 
  summarize(delay=mean(dep_delay>0,na.rm=T)) %>% 
  ggplot(aes(x=hour,y=delay))+geom_col()

# or

flights %>% 
  group_by(hour) %>% 
  summarize(m=mean(dep_delay,na.rm=T),
            sd=sd(dep_delay,na.rm=T),
            low_ci=m-2*sd,
            high_ci=m+2*sd,
            n=n()) %>% 
  ggplot(aes(hour,m,ymin=low_ci,ymax=high_ci))+
  geom_pointrange()
```

4.For each destination, compute the total minutes of delay. For each, flight, compute the proportion of the total delay for its destination.
```{r Grouped Mutates - Exercises 3}

# tital minutes of delay
flights %>% 
  group_by(dest) %>% 
  filter(!is.na(dep_delay)) %>% 
  summarise(tol_mins=sum(dep_delay[dep_delay>0])) %>% 
  arrange(desc(tol_mins)) %>% 
  head()

# total delay proportion
flights %>% 
  filter(!is.na(dep_delay)) %>% 
  group_by(tailnum,dest) %>% 
  summarise(m=mean(dep_delay[dep_delay>0]),
            n=n()) %>% 
  arrange(desc(m)) %>% 
  head()
```

5.Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the delay of the immediately preceding flight.
```{r Grouped Mutates - Exercises 4}
flights %>%
  mutate(new_sched_dep_time = lubridate::make_datetime(year, month, day, hour, minute)) %>%
  arrange(new_sched_dep_time) %>%
  mutate(prev_time = lag(dep_delay)) %>%
  # filter(between(dep_delay, 0, 300), between(prev_time, 0, 300)) %>% # play with this one
  select(origin, new_sched_dep_time, dep_delay, prev_time) %>%
  ggplot(aes(dep_delay, prev_time)) + geom_point(alpha = 1/10) +
  geom_smooth()

# or

flights %>%
  select(year, month, day, hour, dest, dep_delay) %>%
  group_by(dest) %>%
  mutate(lag_delay = lag(dep_delay)) %>%
  arrange(dest) %>%
  filter(!is.na(lag_delay)) %>%
  summarize(cor = cor(dep_delay, lag_delay, use = "complete.obs"),
            n = n()) %>%
  arrange(desc(cor)) %>%
  filter(row_number(desc(cor)) %in% 1:10)
```

Although there is a lot of noise, you can see a sort of straight line going on there. There is also a correlation between the lagged values in many of the destionatinons. So correlation between flights is mostly in specific airports.

6. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?
```{r Grouped Mutates - Exercises 5}
# solution 1
flights %>% 
  group_by(dest) %>% 
  arrange(air_time) %>% 
  slice(1:5) %>% 
  select(tailnum,sched_dep_time,sched_arr_time,air_time) %>% 
  arrange(air_time) %>% 
  head()

# solution 2
flights %>% 
  group_by(dest) %>% 
  mutate(shortest=air_time-min(air_time,na.rm=T)) %>% 
  top_n(1,air_time) %>% 
  arrange(-air_time) %>% 
  select(tailnum,sched_dep_time,sched_arr_time,air_time,shortest) %>% 
  head()
```

7. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers.
```{r Grouped Mutates - Exercises 6}
flights %>% 
  group_by(dest) %>% 
  filter(n_distinct(carrier)>=2) %>% 
  group_by(carrier) %>% 
  summarise(n=n_distinct(dest)) %>% 
  arrange(desc(n))
  # arrange(-n) is the same expression as arrange(desc)
```

# Chapter 7. Exploratory Data Analysis 
## 7.1 Introduction
This chater will cover how to use visualization and transformation to explorer the data in a systematic way, a task that statisticians call exploratory data analysis (EDA)

1. Generate questions about the data
2. Search for answers by visualizing, transforming, and modeling the data
3. Use what we learn to refine the questions and/or generate new questions

## 7.2 Questions
EDA is fundamentally a creative process. AND like most creative processes, the key to asking quality questionsis to generate a large quantitiy of questions. It is difficult to ask revealing questions at the start of your analysis because we do not know what insights are contained in the dataset. 

There is no rule about which questions we should ask to guide the research. However, two typesof questions will always be useful for making discoveries within the data. 

1. What type of variation occurs within the variables?
2. What type of covariation occurs between the variables? 

## 7.3 Variation
Variation is the tendency of the values of a variable to change from measurement to measurement. 

### Visualizing distributions
#### Barplot
Barplot for categorical variables (factors or character vectors)
```{r visualizing distributions Barplot}
ggplot(data=diamonds)+
  geom_bar(aes(x=cut),fill='light gray')

#count heights of tre bars
diamonds %>% 
  dplyr::count(cut) %>% 
  arrange(desc(cut))
```

A variable is continuous if it can take any of an infinite set of oredred values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use histogram.

We can set the width of the intervals in a histogram with the binwidth argument. 

```{r visualizing distributions Histgoram}
ggplot(diamonds)+
  geom_histogram(mapping=aes(x=carat),binwidth=0.3)

# compute this by ggplot2::cut_width()

diamonds %>% 
  count(cut_width(carat,0.5))

smaller <- diamonds %>% 
  filter(carat<3)

ggplot(data=smaller,mapping=aes(x=carat))+
  geom_histogram(binwidth=0.1)+theme_bw()
```

If we want to overlay multiple histograms in the same plot, recommended is geom_freqpoly().
```{r visualizing distributions geom_freqpoly}
ggplot(data=smaller,aes(x=carat,color=cut))+
  geom_freqpoly(binwidth=0.1)
```

#### Typical Values 
Common values and less-common values are useful to come up with useful questions. 
- Which values are the most common? why?
- What values are rare? Why? Does that match the expectation?
- Can we see any unusual patterns? What might explain them?

```{r Typical values}
ggplot(data=smaller, mapping=aes(x=carat))+
  geom_histogram(binwidth = 0.01)
```

In general, clusters of similar values suggest that subgroups exist in the data. To understand the subgroups, ask:
- How are the observations within each clusters different from each other?
- How are the observations in separate clusters different from each other?
- How can we explain or describe the clusters?
- Why might the apprearance of clusters be misleading?

#### Unusual Values
Outliers are observations that are unusual;

We can zoom in to small values of the y-axis with coord_cartesian().
```{r unusual values}
ggplot(diamonds)+
  geom_histogram(mapping=aes(x=y),binwidth=0.5)

# coord_cartesian()
ggplot(diamonds)+
  geom_histogram(aes(x=y),binwidth = 0.5)+
  coord_cartesian(ylim=c(0.50))

usual <- diamonds %>% 
  filter(y<3 | y>20) %>% 
  arrange(y)
usual
```

### Exercises
1. Explore the distribution of each of the x.y and z variables in diamonds. What do you learn? Think abou a diamonds and how we might decide which dimension is the length, width, and depth?
```{r Visualizing distributions exercise 1-1}

head(diamonds)

library(magrittr)
library(tidyr) #gather
library(ggplot2)
library(dplyr) #mutate

diamonds %>% 
  gather(key=dist,vals,x,y,z) %>% 
  mutate(n=n()) %>% 
  ggplot(aes(x=vals,color=dist))+
  geom_freqpoly(bins=100)

cor(diamonds$x, diamonds$y)
```

One thing that is pretty obvious but perhaps hard to grasp at first is that distribution of X and Y are pretty much the same.In fact, the same graph from the above with `bins=30` won't show you the X distribution because it overlaps perfectly. The correlation between the two can be confirmed. 

If we rounded each mm to the closest number, valu-paring x and y yields `mean(with(diamonds,round(x,0)==round(y,0)))` of the values with the same number. So far, the lenfth is directly proportional to the y value. 
```{rVisualizing distributions exercise 1-2}
install.packages('GGally')
diamonds %>% 
  filter(y<30) %>% 
  select(x,y,z) %>% 
  GGally::ggpairs()
```

Yet the relationship between x and y with z is almost flat, as expected. That is ,after excluding 2 diamonds which had unreasonable values. 

2. Expore the distribution of price. Do you discover anything unusual or surprisng? (Hint: Carefully think about the binwidth and make sure you try a wide range of values)
```{r Visualizing distributions exercise 2}

## TO DO: Fixe the Y and X axis to be able to specify the cutting point in the distribution ("http://peterhaschke.com/Code/multiplot.R").

library(purrr) #map function

graph <- map(seq(50,1000,100),
             ~ggplot(diamonds,aes(x=price))+
               geom_histogram(bins=.x)+
               labs(x=NULL,y=NULL)+
               scale_x_continuous(labels=NULL)+
               scale_y_continuous(labels=NULL))
install.packages("Rmisc")
library(Rmisc)
multiplot(plotlist=graph)
```

3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?
```{r Visualizing distributions exercise 3}
library(magrittr)

#diamonds %>%
  # filter(carat %in% c(0.99, 1)) %>%
  # count(carat)
```

4. Compare and contrast coord_cartesian() versus xlim() or ylim() when zooming in on a histogram. What happens if wa leave binwdith unset? What happens if you try and zoon so only half a bar shows? 
```{r Visualizing distributions exercise 4}

library(ggplot2)

# ylim
diamonds %>% 
  ggplot(aes(y))+
  geom_histogram()+
  coord_cartesian(ylim=c(0,50))
# note: how xlim deeited the observation at0

# xlim, ylim
diamonds %>% 
  ggplot(aes(y))+
  geom_histogram()+
  xlim(c(0,60))+
  coord_cartesian(y=c(0,50))

# also note that how xlim and ylim inside coord_carterisan don't exclude the data 
diamonds %>% 
  ggplot(aes(y))+
  geom_histogram(bins=30)+
  coord_cartesian(xlim=c(0,60),ylim=c(0,50))
```

http://r4ds.had.co.nz/exploratory-data-analysis.html

## 7.4 Missing Values
If we have encontered unusual values in te dataset, and simply want to move on to the rest of the analysis, we have two options.

1. Drop the entire row with the strange values 
```{r}
library(dplyr) # between()
diamonds2 <- diamonds %>% 
  filter(between(y,3,20))
```

This is not recommended, because one measurement is invlaid, doesn't mean all the measurements are. Additionally, if we have low quality data, by time that we have applied this approach to every variable, we might find that we don't have any data left.

2. Instead, it is recommended replacing the unusual values with missing values. The easiest way to do this is to use `mutate()` to replace variable with a modified copy. We can use the `ifelse()` function to replace unusual values with `NA`: 
```{r}
library(Amelia)
missmap(diamonds)

diamonds2 <- diamonds %>% 
  mutate(y=ifelse(y<3 | y>20, NA,y))
```

`ifelese()` has three arguments.
The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when `test` is `TRUE`, and the value of the third argument, `no`, when it is false. 

Line R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It's not obvious where we should plot missing values, so ggplot2 doesn't include them in the plot, but it does warn that they haven been removed: 
```{r}
ggplot(data=diamonds2,mapping=aes(x=x,y=y))+
  geom_point()
```

To suppress that warning, set `n.rm=T`.
```{r}
ggplot(diamonds2,aes(x=x,y=y))+geom_point(na.rm=T)
```

Other time we want to understand what makes obervations with missing values different to observations with recored values. For example, in `nycfilights13::flights`, missing values in the `dep_time` variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and cancelled times. We can do this by making a new variable with `is.na()`.

```{r}
nycflights13::flights %>% 
  mutate(
    cancelled=is.na(dep_time),
    sched_hour=sched_dep_time %/% 100,
    sched_min=sched_dep_time %% 100,
    sched_dep_time=sched_hour+sched_min/60
  ) %>% 
  ggplot(aes(sched_dep_time))+
  geom_freqpoly(aes(color=cancelled),binwidth=1/4)
```

However, this plot isn't great because there are many more non-cancelled filights than cancelled flights. In the next section we'll explore some techniques for improving this comparison.

### 7.4.1 Missing value Exercise
1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?
```{r}
missmap(diamonds)

diamonds %>% 
  ggplot(aes(price))+
  geom_histogram(bins=1000)
```

In a hostogram, they simply leave a gap in the distribution, as in the gap in the above histogram of price. For the bardplot, the function removes the `NA` value. 
```{r}
mtcars[1,2] <- NA
head(mtcars)

mtcars %>% 
  ggplot(aes(cyl))+
  geom_bar()
```

2. What does `na.rm` = TRUE do in `mean()` and `sum()`?
It removes the `NA` from the calculations. 

## 7.5 Covarition
IF variation descrbies the behavior within a variable, covariation describes the behaviour between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The besy way to spot covariation is to visualize the relationship between two or more variables. Hwo do we do that should again depend on the type of variables involved.

### 7.5.1 A categorical and continuous variable
The default appearance of `geom_freqpoly()` is not that useful for that sort of comparison because the height is given by the count.
```{r}
ggplot(diamonds,aes(x=price))+
  geom_freqpoly(aes(color=cut),binwidth=500)
```

It's hard to see the difference in distribution because the overall counts differe so much: 
```{r}
ggplot(diamonds)+
  geom_bar(aes(x=cut))
```

To make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we'll display density, which is the count standardized so that the area under frequent polygon is one. 
```{r}
ggplot(diamonds,aes(x=price,y=..density..))+
  geom_freqpoly(aes(color=cut),binwidth=500)
```

There is something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price. But maybe thats because frequency polygons are little hard to interpret.

Another alternative to display the distribution of continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. 

```{r}
ggplot(diamonds,aes(x=cut,y=price))+
  geom_boxplot()+geom_point(aes(color='red'),size=0.1)
```

Many categorical variables don't have such an intrinsic order, so we might want to reorder them to make a more informative display. Onew way to do that is with the `reorder()` function.

Example
Take the `class` variable in the `mpg` dataset. We might be intersted to know how highway mileage varies across classes:
```{r}
# default
ggplot(data=mpg,mapping=aes(x=class,y=hwy))+
  geom_boxplot()

# reorder()
ggplot(data=mpg)+
  geom_boxplot(mapping=aes(x=reorder(class,hwy),y=hwy))

ggplot(data=mpg)+
  geom_boxplot(mapping=aes(x=reorder(class,hwy,FUN=median),y=hwy))
```

If you have long variable names, `geom_boxplot()` will work better if we flip it 90. We can do that with `coord_flip()`
```{r}
ggplot(data=mpg)+
  geom_boxplot(mapping=aes(x=reorder(class,hwy,FUN=median),y=hwy))+
  coord_flip()
```

#### 7.5.1.1 Exercise

1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights.
```{r}
library(nycflights13)
fl <- 
  flights %>% 
  mutate(
    cancelled=is.na(dep_time),
    sched_hour=sched_dep_time %/% 100,
    sched_min=sched_dep_time %% 100,
    sched_dep_time=sched_hour+sched_min/60
  )

fl %>% 
  ggplot(aes(sched_dep_time,..density..,color=cancelled))+
  geom_freqpoly(binwidth=1/2)

fl %>%
  ggplot(aes(sched_dep_time, colour = cancelled)) +
  geom_density()

fl %>%
  ggplot(aes(cancelled, sched_dep_time)) +
  geom_boxplot()
```


2. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?
```{r}
# install.packages("arm")
library(arm) # display()
display(lm(price~.,data=diamonds),detail=T)
# In a ditty way, carat

# Let's confirm the varoatopn on carat fpr cit
diamonds %>% 
  ggplot(aes(cut,carat))+
  geom_boxplot()

# It looks like it's weakly negatively correlated, so the fair diamonds having the greater carat. 

diamonds %>% 
  ggplot(aes(carat,color=cut))+
  geom_density(possition="dodge")

# It looks fair diamons have the highest average carat but only by a little
diamonds %>%
  group_by(cut) %>% 
  summarise(cor(carat,price))
```

It does look like the carat and price are highly correlated between, as well as within, the quality of the diamond.

3. Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()?
```{r}
# install.packages('ggstance')
library(ggstance)

diamonds %>%
  ggplot(aes(cut, carat)) +
  geom_boxplot() +
  coord_flip()

diamonds %>%
  ggplot(aes(carat, cut)) +
  geom_boxploth()
```

4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots?
```{r}
# install.packages('lvplot')
library(lvplot)

p <- ggplot(diamonds,aes(cut,price,color=..LV..))
p+geom_lv()

p <- ggplot(diamonds, aes(cut, carat, fill = ..LV..))
p + geom_lv()
```

This plot is useful for having a more detailed description of the tails in a distribution. This works because each particular plot has both height and width. So, for example, we can see that upper tail for `Fai` has more values that upper tail for `Ideal`. In a similar line, the distribution of `Ideal` is descreasing both in the number of carats as well as in the number of outliers as it increases towards the upper tail. The information is very difficult to get visually with a boxplot.

5. Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method?
```{r}
diamonds %>%
  ggplot(aes(cut, price)) +
  geom_violin()

diamonds %>%
  ggplot(aes(price)) +
  geom_histogram() +
  facet_wrap(~ cut, scale = "free_y", nrow = 1)

diamonds %>%
  ggplot(aes(price)) +
  geom_freqpoly(aes(colour = cut))

diamonds %>%
  ggplot(aes(price)) +
  geom_freqpoly()
```

6. If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.

### 7.5.2 Two categorical variables
To visualize the covariation between categorical variables, you'll need to count the number of observations for each combination. One way to do that is to rely on the built-in `geom_count()`:

```{r}
ggplot(diamonds)+
  geom_count(mapping=aes(x=cut,y=color))
```

Another approach is to compute the count with dplyr:
```{r}
# library(dplyr)
# diamonds %>%
  # count(color,cut)
```

Then visualize with `geom_tile()` and the fill aesthetic:
```{r}
# dataset overview
head(diamonds)

# diamonds %>% 
#   count(color,cut) %>% 
#   ggplot(aes(x=color,y=cut))+
#   geom_tile(aes(fill=n))
```

If the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, we might want to try the d3heatmap or heatmaply packages, which create interactive plots.

#### 7.5.2.1 Exercises
1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut?
```{r}
# By calculating percentages and also showing the n
diamonds %>% 
  count(color,cut) %>% 
  group_by(color) %>%
  mutate(perc=n/sum(n)) %>% 
  ggplot(aes(color,cut,fill=perc))+
  geom_tile()
```

2. Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?

One thing that makes it extremely difficult read is that it is difficult to see differences in dep_delay because the higher values are driving the whole color palette upwards. Also, many dest have missing values on some months. Two solutions could be done: 
- exclude dest with missing values for now and summarise
- standardize or recale the dep_delay so that we can spot differences
```{r}
install.packages("viridis")
library(viridis)

flights %>%
  ggplot(aes(x = month, y = dest, fill = dep_delay)) +
  geom_tile()

flights %>%
  mutate(tot_delay = dep_delay + arr_delay) %>%
  filter(tot_delay > 0) %>%
  group_by(dest, month) %>%
  summarize(dep_del_dev = mean(tot_delay, na.rm = T)) %>%
  filter(n() == 12) %>%
  ungroup() %>%
  ggplot(aes(x = factor(month), y = fct_reorder(dest, dep_del_dev), fill = dep_del_dev)) +
  geom_tile() +
  scale_fill_viridis()

```

3. Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above?

Because the cut is ordered giving the impression of scatterplot-type of intuition. Also, it's better to have names that we interpret constantly (and are a bit lenfthy) on they axis.
```{r}
# install.packages('magrittr')
# install.packages('ggplot2')
# install.packages('tidyverse')
# install.packages("plyr") #count()

library(magrittr)
library(ggplot2)
library(plyr)

head(diamonds)
diamonds %>%
  count(color, cut) %>%
  ggplot(aes(x=color, y = cut)) +
  geom_tile(aes(fill = n))
```

## 7.5.3 Two continuous variables
We have seen one great way to visualize the covariation between two continuous variables: draw a scatterplot with `geom_point()`. You can see covariation as a pattern in the points.

Example: exponential relationship
```{r}
ggplot(diamonds)+
  geom_point(aes(x=carat,y=price))
```

Scatterplots become less useful as the size of the dataset grows, points begin to oveplot and pile up into areas of uniform black, which can be fixed by `alpha` aesthetic to add transparency.
```{r}
ggplot(diamonds)+
  geom_point(aes(carat,price),alpha=0.01)
```

However, tranparency can be challenging for very large datasets. Another solution is to use bin. 

`geom_bin2d()` and `geom_hex()` divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. 
- `geom_bin2d`: creates rectangular bins
- `geom_hex`: creates hexagonal bins
```{r}
par(mfrow=c(2,1))

ggplot(smaller)+
  geom_bin2d(aes(x=carat,y=price))

# install.packages("hexbin")
ggplot(smaller)+
  geom_hex(aes(carat,price))
```

Another option is to bin one continuous variable so it acts like a categorical variable. Then we can use one of the techniques for visualizing the combination of a categorical and a conttinuous variable. 
```{r}
ggplot(smaller,aes(x=carat,y=price))+
  geom_boxplot(aes(group=cut_width(carat,0.1)))
```

`cut_width(x,width)`, as used above, divides x into bins of width `width`. By default, boxplots look roughly the same (aprt from number of outliers) regardless of how many obs the are, so its difficult to tell that each boxplot summarises a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with `varwidth=TRUE`.

Another approach is to display approximately the same number of points in each bin. That's the job of `cut_number`:
```{r}
ggplot(smaller,aes(carat,price))+
  geom_boxplot(aes(group=cut_number(carat,20)))
```

#### 7.5.3.1 Exercises
1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price?
```{r}
ggplot(diamonds,aes(price,color=cut_width(carat,0.3)))+
  geom_freqpoly()
```

So cut_number makes N groups havin the same number of observation in each group. Here there's no contextual or substantive reason for different coding. 

cut_width is more substancial. You make groups based on the metric of the variables.

In fact, there's a tension between the two. The idea is that you reach an equilibrium between the two; categoris which have substantial meaning but also have reasonable sample sizes.
```{r}
# Here we get 10 groups with the same sample size.
ggplot(data = diamonds, aes(x=cut_number(price, 10), y=carat)) +
  geom_boxplot()

# The relationship seems a bit exponential with little differences between the smaller groups.
# But with 10 groups based on substantive metrics:
ggplot(data = diamonds, aes(x=cut_width(price, 2000), y=carat)) +geom_boxplot()
```

2. Visualise the distribution of carat, partitioned by price.
```{r}
ggplot(diamonds,aes(carat,y=..density..,color=cut_width(price,2000)))+
  geom_freqpoly()
```

3. How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you?
```{r}
head(diamonds)
summary(diamonds$carat)

diamonds %>%
  filter(between(carat, 0, 2.5)) %>%
  mutate(carat = cut_width(carat, 1)) %>%
  ggplot(aes(price)) +
           geom_histogram() +
           facet_wrap(~ carat)
```

4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price.
```{r}
diamonds %>%
  filter(between(carat, 0, 2.5)) %>%
  mutate(carat = cut_width(carat, 1)) %>%
  ggplot(aes(cut, price)) +
  geom_boxplot() +
  scale_y_log10() +
  facet_wrap(~ carat)
```


## 7.6 Patterns and models
Pattern in the data provide clues about relatinships. IF a systematic relatinship exist between two viariables, it will apear as a pattern in the data. IF we spot a patter, ask yourself:
- could this pattern be due to concidence (random chance)?
- How can you describe the relationship implied by the pattern?
- How strong is the relatinship implied by the pattern?
- What other variables might affect the relatinship?
- Does the relatinship change if we look at individual subgroups of the data?

```{r}
ggplot(faithful)+
  geom_point(mapping=aes(x=eruptions,y=waiting))
```

Patterns provide one of the most useful tools for data scientist because they reveal covariation. IF we think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it.

Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It's hard to understand the relatinship between cut and price, because cut and carat, and carat and price are tighly related. 

It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts `price` from `carat` and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

```{r}
library(modelr)

mod <- lm(log(price)~log(carat),data=diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid=exp(resid))

ggplot(data=diamonds2)+
  geom_point(aes(x=carat,y=resid))
```

Once we have removed the strong relation between carat and price, we can see what we would expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.
```{r}
library(ggplot2)
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

You’ll learn how models, and the modelr package, work in the final part of the book, model. We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand.

## 7.7 ggplot2 calls
As we move on from these introductory chapters, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning:

```{r}
ggplot(data=faithful,aes(x=eruptions))+
  geom_freqpoly(binwidth=0.25)
```

Someimes,we’ll turn the end of a pipeline of data transformation into a plot. Watch for the transition from %>% to +. I wish this transition wasn’t necessary but unfortunately ggplot2 was created before the pipe was discovered.  
```{r}
library(magrittr)
library(dplyr) #count()

diamonds %>% 
  count(cut,clarity) %>% 
  ggplot(aes(clarity,cut,fill=n))+
  geom_tile()
```














