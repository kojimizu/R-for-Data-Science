---
title: "III. Program"
output:
  word_document: default
  html_document: default
---

# III. Program
# Chapter 17: Introduction
In this part of the book, you’ll improve your programming skills. Programming is a cross-cutting skill needed for all data science work: you must use a computer to do data science; you cannot do it in your head, or with pencil and paper.

Programming produces code, and code is a tool of communication. Obviously code tells the computer what you want it to do. But it also communicates meaning to other humans. Thinking about code as a vehicle for communication is important because every project you do is fundamentally collaborative. Even if you’re not working with other people, you’ll definitely be working with future-you! Writing clear code is important so that others (like future-you) can understand why you tackled an analysis in the way you did. That means getting better at programming also involves getting better at communicating. Over time, you want your code to become not just easier to write, but easier for others to read.

Writing code is similar in many ways to writing prose. One parallel which I find particularly useful is that in both cases rewriting is the key to clarity. The first expression of your ideas is unlikely to be particularly clear, and you may need to rewrite multiple times. After solving a data analysis challenge, it’s often worth looking at your code and thinking about whether or not it’s obvious what you’ve done. If you spend a little time rewriting your code while the ideas are fresh, you can save a lot of time later trying to recreate what your code did. But this doesn’t mean you should rewrite every function: you need to balance what you need to achieve now with saving time in the long run. (But the more you rewrite your functions the more likely your first attempt will be clear.)

In the following four chapters, you’ll learn skills that will allow you to both tackle new programs and to solve existing problems with greater clarity and ease:

In pipes, you will dive deep into the pipe, %>%, and learn more about how it works, what the alternatives are, and when not to use it.

Copy-and-paste is a powerful tool, but you should avoid doing it more than twice. Repeating yourself in code is dangerous because it can easily lead to errors and inconsistencies. Instead, in functions, you’ll learn how to write functions which let you extract out repeated code so that it can be easily reused.

As you start to write more powerful functions, you’ll need a solid grounding in R’s data structures, provided by vectors. You must master the four common atomic vectors, the three important S3 classes built on top of them, and understand the mysteries of the list and data frame.

Functions extract out repeated code, but you often need to repeat the same actions on different inputs. You need tools for iteration that let you do similar things again and again. These tools include for loops and functional programming, which you’ll learn about in iteration.

## 17.1 For further study
The goal of these chapters is to teach you the minimum about programming that you need to practice data science, which turns out to be a reasonable amount. Once you have mastered the material in this book, I strongly believe you should invest further in your programming skills. Learning more about programming is a long-term investment: it won’t pay off immediately, but in the long term it will allow you to solve new problems more quickly, and let you reuse your insights from previous problems in new scenarios.

To learn more you need to study R as a programming language, not just an interactive environment for data science. We have written two books that will help you do so:

Hands on Programming with R, by Garrett Grolemund. This is an introduction to R as a programming language and is a great place to start if R is your first programming language. It covers similar material to these chapters, but with a different style and different motivation examples (based in the casino). It’s a useful complement if you find that these four chapters go by too quickly.

Advanced R by Hadley Wickham. This dives into the details of R the programming language. This is a great place to start if you have existing programming experience. It’s also a great next step once you’ve internalised the ideas in these chapters. You can read it online at http://adv-r.had.co.nz.

# Chapter 18 Pipes
## 18.1 Introduction
Pipes are a powerful tool for clearly expressing a sequence of multiple operations. So far, you’ve been using them without knowing how they work, or what the alternatives are. Now, in this chapter, it’s time to explore the pipe in more detail. You’ll learn the alternatives to the pipe, when you shouldn’t use the pipe, and some useful related tools.

### 17.1.1 Prerequisites
The pipe, %>%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %>% for you automatically, so you don’t usually load magrittr explicitly. Here, however, we’re focussing on piping, and we aren’t loading any other packages, so we will load it explicitly.
```{r}
library(tidyverse)
library(purrr)
library(magrittr)

# install.packages("pryr")
library(pryr)
```

## 18.2 Piping alternatives
The point of the pipe is to help you write code in a way that is easier to read and understand. To see why the pipe is so useful, we’re going to explore a number of ways of writing the same code. Let’s use code to tell a story about a little bunny named Foo Foo:
>Little bunny Foo Foo
Went hopping through the forest
Scooping up the field mice
And bopping them on the head

This is a popular Children’s poem that is accompanied by hand actions.We’ll start by defining an object to represent little bunny Foo Foo:

```{r 18.2-1}
# foo_foo <- little_bunny()
```

And we’ll use a function for each key verb: `hop()`, `scoop()`, and `bop()`. Using this object and these verbs, there are (at least) four ways we could retell the story in code:

1. Save each intermediate step as a new object.
2. Overwrite the original object many times.
3. Compose functions.
4. Use the pipe.

We’ll work through each approach, showing you the code and talking about the advantages and disadvantages.

### 18.2.1 Intermediate steps
The simplest approach is to save each step as a new object:

```{r 18.2.1-1}
# foo_foo_1 <- hop(foo_foo,through=forest)
# foo_foo_2 <- scoop(foo_foo_1, up = field_mice)
# foo_foo_3 <- bop(foo_foo_2, on = head)
```

The main downside of this form is that it forces you to name each intermediate element. If there are natural names, this is a good idea, and you should do it. But many times, like this in this example, there aren’t natural names, and you add numeric suffixes to make the names unique. That leads to two problems:

1. The code is cluttered with unimportant names
2. You have to carefully increment the suffix on each line.

Whenever I write code like this, I invariably use the wrong number on one line and then spend 10 minutes scratching my head and trying to figure out what went wrong with my code.

You may also worry that this form creates many copies of your data and takes up a lot of memory. Surprisingly, that’s not the case. First, note that proactively worrying about memory is not a useful way to spend your time: worry about it when it becomes a problem (i.e. you run out of memory), not before. Second, R isn’t stupid, and it will share columns across data frames, where possible. Let’s take a look at an actual data manipulation pipeline where we add a new column to `ggplot2::diamonds`:
```{r 18.2.1-2}
diamonds <- ggplot2::diamonds
diamonds2 <- diamonds %>% 
  dplyr::mutate(price_per_carat=price/carat)

pryr::object_size(diamonds)
pryr::object_size(diamonds2)
pryr::object_size(diamonds,diamonds2)
```

`pryr::object_size()` gives the memory occupied by all of its arguments. The results seem counterintuitive at first:

- `diamonds` takes up 3.46 MB,
- `diamonds2` takes up 3.89 MB,
- `diamonds` and `diamonds2` together take up 3.89 MB!

How can that work? Well, `diamonds2` has 10 columns in common with `diamonds`: there’s no need to duplicate all that data, so the two data frames have variables in common. These variables will only get copied if you modify one of them. In the following example, we modify a single value in `diamonds$carat`. That means the `carat` variable can no longer be shared between the two data frames, and a copy must be made. The size of each data frame is unchanged, but the collective size increases:
```{r 18.2.1-3}
diamonds$carat[1] <- NA
pryr::object_size(diamonds)
pryr::object_size(diamonds2)
pryr::object_size(diamonds,diamonds2)
```

(Note that we use `pryr::object_size()` here, not the built-in `object.size()`. `object.size()` only takes a single object so it can’t compute how data is shared across multiple objects.)

### 18.2.2 Overwrite the original
Instead of creating intermediate objects at each step, we could overwrite the original object:
```{r 18.2.2-1}
# foo_foo <- hop(foo_foo, through = forest)
# foo_foo <- scoop(foo_foo, up = field_mice)
# foo_foo <- bop(foo_foo, on = head)
```

This is less typing (and less thinking), so you’re less likely to make mistakes. However, there are two problems:

1. Debugging is painful: if you make a mistake you’ll need to re-run the complete pipeline from the beginning.
2. The repetition of the object being transformed (we’ve written foo_foo six times!) obscures what’s changing on each line.

### 18.2.3 Function composition
Another approach is to abandon assignment and just string the function calls together:
```{r 18.2.3-1}
# bop(
#   scoop(
#     hop(foo_foo, through = forest),
#     up = field_mice
#   ), 
#   on = head
# )
```

Here the disadvantage is that you have to read from inside-out, from right-to-left, and that the arguments end up spread far apart (evocatively called the dagwood sandwhich problem). In short, this code is hard for a human to consume.

### 18.2.4 Use the pipe
Finally, we can use the pipe:
```{r 18.2.4-1}
# foo_foo %>%
#   hop(through = forest) %>%
#   scoop(up = field_mice) %>%
#   bop(on = head)
```

This is my favourite form, because it focusses on verbs, not nouns. You can read this series of function compositions like it’s a set of imperative actions. Foo Foo hops, then scoops, then bops. The downside, of course, is that you need to be familiar with the pipe. If you’ve never seen `%>%` before, you’ll have no idea what this code does. Fortunately, most people pick up the idea very quickly, so when you share your code with others who aren’t familiar with the pipe, you can easily teach them.

The pipe works by performing a “lexical transformation”: behind the scenes, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. When you run a pipe like the one above, magrittr does something like this:

```{r 18.2.4-2}
# my_pipe <- function(.) {
#   . <- hop(., through = forest)
#   . <- scoop(., up = field_mice)
#   bop(., on = head)
# }
# my_pipe(foo_foo)
```

This means that the pipe won’t work for two classes of functions:

1. Functions that use the current environment. For example, `assign()` will create a new variable with the given name in the current environment:
```{r 18.2.4-3}
assign("x",10)
x

"x" %>% assign(100)
x
```

The use of assign with the pipe does not work because it assigns it to a temporary environment used by `%>%`. If you do want to use assign with the pipe, you must be explicit about the environment:
```{r 18.2.4-4}
env <- environment()
"x" %>% assign(100,envir=env)
x
```

Other functions with this problem include `get()` and `load()`.

2. Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behaviour.

One place that this is a problem is `tryCatch()`, which lets you capture and handle errors:
```{r}
# tryCatch(stop("!"),error=function(e)"An error")
# 
# stop("!") %>% 
#   tryCatch(error = function(e) "An error")
# # Error in eval(lhs, parent, parent) : !
```

There are a relatively wide class of functions with this behaviour, including `try()`, `suppressMessages()`, and `suppressWarnings()` in base R.

## 18.3 When not to use the pipe
The pipe is a powerful tool, but it’s not the only tool at your disposal, and it doesn’t solve every problem! Pipes are most useful for rewriting a fairly short linear sequence of operations. I think you should reach for another tool when:

- Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent.

- You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe.

- You are starting to think about a directed graph with a complex dependency structure. Pipes are fundamentally linear and expressing complex relationships with them will typically yield confusing code.

## 18.4 Other tools from magrittr
All packages in the tidyverse automatically make %>% available for you, so you don’t normally load magrittr explicitly. However, there are some other useful tools inside magrittr that you might want to try out:

- When working with more complex pipes, it’s sometimes useful to call a function for its side-effects. Maybe you want to print out the current object, or plot it, or save it to disk. Many times, such functions don’t return anything, effectively terminating the pipe.

To work around this problem, you can use the “tee” pipe. %T>% works like %>% except that it returns the left-hand side instead of the right-hand side. It’s called “tee” because it’s like a literal T-shaped pipe.


## 18.4 Other tools from magrittr
All packages in the tidyverse automatically make `%>%` available for you, so you don’t normally load magrittr explicitly. However, there are some other useful tools inside magrittr that you might want to try out:

- When working with more complex pipes, it’s sometimes useful to call a function for its side-effects. Maybe you want to print out the current object, or plot it, or save it to disk. Many times, such functions don’t return anything, effectively terminating the pipe.

To work around this problem, you can use the “tee” pipe. `%T>%` works like `%>%` except that it returns the left-hand side instead of the right-hand side. It’s called “tee” because it’s like a literal T-shaped pipe.
```{r 18.4-1}
rnorm(100) %>% 
  matrix(ncol=2) %>% 
  plot() %>% 
  str()

rnorm(100) %>% 
  matrix(ncol=2) %>% 
  plot() %>% 
  str()

ndist <- rnorm(100000)
hist(ndist)
```

- If you’re working with functions that don’t have a data frame based API
(i.e. you pass them individual vectors, not a data frame and expressions to be evaluated in the context of that data frame), you might find `%$%` useful. It “explodes” out the variables in a data frame so that you can refer to them explicitly. This is useful when working with many functions in base R:
```{r 18.4-2}
mtcars %$%
  cor(disp, mpg)
```

- For assignment magrittr provides the `%<>%` operator which allows you to replace code like:
```{r 18.4-3}
mtcars <- mtcars %>% 
  transform(cyl=cyl*2)
```

with 
```{r 18.4-4}
mtcars %<>% transform(cyl=cyl*2)
```

I’m not a fan of this operator because I think assignment is such a special operation that it should always be clear when it’s occurring. In my opinion, a little bit of duplication (i.e. repeating the name of the object twice) is fine in return for making assignment more explicit.

# Chapter 19 Functions
## 19.1 Introduction
One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste:

1. You can give a function an evocative name that makes your code easier to understand.
2. As requirements change, you only need to update code in one place, instead of many.
3. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).

Writing good functions is a lifetime journey. Even after using R for many years I still learn new techniques and better ways of approaching old problems. The goal of this chapter is not to teach you every esoteric detail of functions but to get you started with some pragmatic advice that you can apply immediately.

As well as practical advice for writing functions, this chapter also gives you some suggestions for how to style your code. Good code style is like correct punctuation. Youcanmanagewithoutit, but it sure makes things easier to read! As with styles of punctuation, there are many possible variations. Here we present the style we use in our code, but the most important thing is to be consistent.

## 19.1.1 Prerequisites
The focus of this chapter is on writing functions in base R, so you won’t need any extra packages.

## 19.2 When should you write a function?
You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do?
```{r 19.2-1}
df <- tibble::tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df

df$a <- (df$a - min(df$a, na.rm = TRUE)) / 
  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
df$b <- (df$b - min(df$b, na.rm = TRUE)) / 
  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))
df$c <- (df$c - min(df$c, na.rm = TRUE)) / 
  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))
df$d <- (df$d - min(df$d, na.rm = TRUE)) / 
  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))
```

You might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? I made an error when copying-and-pasting the code for `df$b`: I forgot to change an `a` to a `b`. Extracting repeated code out into a function is a good idea because it prevents you from making this type of mistake.

To write a function, you need to first analyze the code. How many inputs does it have?
```{r 19.2-2}
(df$a - min(df$a, na.rm = TRUE)) /
  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
```

This code only has one input: `df$a`. (If you’re surprised that `TRUE` is not an input, you can explore why in the exercise below.) To make the inputs more clear, it’s a good idea to rewrite the code using temporary variables with general names. Here this code only requires a single numeric vector, so I’ll call it `x`:
```{r 19.2-3}
x <- df$a
(x - min(x, na.rm = T)) / (max(x, na.rm = T)-min(x, na.rm = T))
```

There is some duplication in this code. We’re computing the range of the data three times, so it makes sense to do it in one step:
```{r 19.2-4}
rng <- range(x, na.rm = T)
(x-rng[1])/(rng[2]-rng[1])
```

Pulling out intermediate calculations into named variables is a good practice because it makes it more clear what the code is doing. Now that I’ve simplified the code, and checked that it still works, I can turn it into a function:
```{r 19.2-5}
rescale01 <- function(x){
  rng <- range(x, na.rm = T)
  (x-rng[1])/(rng[2]-rng[1])
}
rescale01(c(0,5,10))
```

There are three key steps to creating a new function:
1. You need to pick a name for the function. Here I’ve used `rescale01` because this function rescales a vector to lie between 0 and 1.
2. You list the inputs, or arguments, to the `function` inside function. Here we have just one argument. If we had more the call would look like `function(x, y, z)`.
3. You place the code you have developed in body of the function, a { block that immediately follows `function(...)`.

Note that overall process:  I only made the function after I’d figured out how to make it work with a simple input. It’s easier to start with working code and turn it into a function; it’s harder to create a function and then try to make it work.

At this point it’s a good idea to check your function with a few different inputs:
```{r 19.2-6}
rescale01(c(-10,0,10))
rescale01(c(1,2,3,NA,5))
```

As you write more and more functions you’ll eventually want to convert these informal, interactive tests into formal, automated tests. That process is called unit testing. Unfortunately, it’s beyond the scope of this book, but you can learn about it in http://r-pkgs.had.co.nz/tests.html.

We can simplify the original example now that we have a function:
```{r 19.2-7}
df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)
```

Compared to the original, this code is easier to understand and we’ve eliminated one class of copy-and-paste errors. There is still quite a bit of duplication since we’re doing the same thing to multiple columns. We’ll learn how to eliminate that duplication in iteration, once you’ve learned more about R’s data structures in vectors.

Another advantage of functions is that if our requirements change, we only need to make the change in one place. For example, we might discover that some of our variables include infinite values, and `rescale01()` fails:
```{r 19.2-8}
x <- c(1:10,Inf)
rescale01(x)
```

Because we’ve extracted the code into a function, we only need to make the fix in one place:
```{r 19.2-9}
rescale01 <- function(x){
  rng <- range(x,na.rm=T,finite=T)
  (x-rng[1])/(rng[2]-rng[1])
}
rescale01(x)
```

This is an important part of the “do not repeat yourself” (or DRY) principle. The more repetition you have in your code, the more places you need to remember to update when things change (and they always do!), and the more likely you are to create bugs over time.

### 19.2.1 Practice
1. Why is `TRUE` not a parameter to `rescale01()`? What would happen if x contained a single missing value, and `na.rm` was `FALSE`?
```{r}
rescale01 <- function(x){
  rng <- range(x,na.rm=F)
  (x-rng[1])/(rng[2]-rng[1])
}
rescale01(c(-1,0,5,20,NA))
```

Everything is NA! We should set an argument to control the `TRUE` or `FALSE` of `range`.

2. In the second variant of `rescale01()`, infinite values are left unchanged. Rewrite `rescale01()` so that `-Inf` is mapped to 0, and `Inf` is mapped to 1.
```{r}
rescale01 <- function(x){
  rng <- range(x,na.rm=T,finite=T)
  (x-rng[1])/(rng[2]-rng[1])
  x[x==Inf]<-1
  x[x==-Inf] <- 0
  x
}

rescale01(c(Inf,-Inf,0:5,NA))
```

3. Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative?
```{r}
mean(is.na(x))
x / sum(x, na.rm = TRUE)
sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
```

```{r}
prop_miss <- function(x){
  mean(is.na(x))
}

my_mean <- function(x){
  x/sum(x,na.rm=T)
}

my_var <- function(x){
  sd(x,na.rm=T)/mean(x,na.rm=T)
}
```


4. Follow http://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector.

5. Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.
```{r}
both_na <- function(x,y){
  stopifnot(length(x)==length(y))
  which(is.na(x) & is.na(y))
}
both_na(c(1,2,NA,2,NA),c(1,2,3,4,NA))
```

6. What do the following functions do? Why are they useful even though they are so short?
```{r}
is_directory <- function(x) file.info(x)$isdir
is_readable <- function(x) file.access(x, 4) == 0
```

## 19.3 Functions are for humans and computers

It’s important to remember that functions are not just for the computer, but are also for humans. R doesn’t care what your function is called, or what comments it contains, but these are important for human readers. This section discusses some things that you should bear in mind when writing functions that humans can understand.

The name of a function is important. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names.

Generally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. `mean()` is better than `compute_mean()`), or accessing some property of an object (i.e. `coef()` is better than `get_coefficients()`). A good sign that a noun might be a better choice is if you’re using a very broad verb like “get”, “compute”, “calculate”, or “determine”. Use your best judgement and don’t be afraid to rename a function if you figure out a better name later.

If your function name is composed of multiple words, I recommend using “snake_case”, where each lowercase word is separated by an underscore. camelCase is a popular alternative. It doesn’t really matter which one you pick, the important thing is to be consistent: pick one or the other and stick with it. R itself is not very consistent, but there’s nothing you can do about that. Make sure you don’t fall into the same trap by making your code as consistent as possible.

If you have a family of functions that do similar things, make sure they have consistent names and arguments. Use a common prefix to indicate that they are connected. That’s better than a common suffix because autocomplete allows you to type the prefix and see all the members of the family.
```{r}
# Good
# input_select()
# input_checkbox()
# input_text()
```

A good example of this design is the stringr package: if you don’t remember exactly which function you need, you can type `str_` and jog your memory.

Where possible, avoid overriding existing functions and variables. It’s impossible to do in general because so many good names are already taken by other packages, but avoiding the most common names from base R will avoid confusion.

## 19.4 Conditional execution
An `if` statement allows you to conditionally execute code. 
It looks like this:
```{r 19.4-1}
# if (condition) {
  # code executed when condition is TRUE
# } else {
  # code executed when condition is FALSE
# }
```

To get help on `if` you need to surround it in backticks: `"?if`. The help isn’t particularly helpful if you’re not already an experienced programmer, but at least you know how to get to it!

Here’s a simple function that uses an `if` statement. The goal of this function is to return a logical vector describing whether or not each element of a vector is named.
```{r 19.4-2}
has_name <- function(x){
  nms <- names(x)
  if(is.null(nms)){
    rep(FALSE,length(x))
  }else {
    !is.na(nms) & nms !=""
  }
}
```

This function takes advantage of the standard return rule: a function returns the last value that it computed. Here that is either one of the two branches of the if statement.

### 19.4.1 Conditions
The `condition` must evaluate to either `TRUE` or `FALSE`. If it’s a vector, you’ll get a warning message; if it’s an `NA`, you’ll get an error. Watch out for these messages in your own code:
```{r 19.4.1-1}
# if (c(TRUE,FALSE)){}
#> Warning in if (c(TRUE, FALSE)) {: the condition has length > 1 and only the
#> first element will be used
#> NULL

# if (NA) {}
```

You can use `||` (or) and `&&` (and) to combine multiple logical expressions. These operators are “short-circuiting”: as soon as `||` sees the first `TRUE` it returns `TRUE` without computing anything else. As soon as `&&` sees the first `FALSE` it returns `FALSE`. You should never use `|` or `&` in an if statement: these are vectorised operations that apply to multiple values (that’s why you use them in `filter()`). If you do have a logical vector, you can use `any()` or `all()` to collapse it to a single value.
```{r}
identical(0L,0)
x <- sqrt(2)^2
x==2
x-2
```

Instead use `dplyr::near()` for comparisons, as described in comparisons.

And remember, `x == NA` doesn’t do anything useful!

### 19.4.2 Multiple conditions
You can chain multiple if statement together:
```{r 19.4.2-1}
# if (this) {
#   # do that
# } else if (that) {
#   # do something else
# } else {
#   # 
# }
```

But if you end up with a very long series of chained `if` statements, you should consider rewriting. One useful technique is the `switch()` function. It allows you to evaluate selected code based on position or name.
```{r 19.4.2-2}
#> function(x, y, op) {
#>   switch(op,
#>     plus = x + y,
#>     minus = x - y,
#>     times = x * y,
#>     divide = x / y,
#>     stop("Unknown op!")
#>   )
#> }
```

Another useful function that can often eliminate long chains of if statements is `cut()`. It’s used to discretise continuous variables.

### 19.4.3 Code style
Both `if` and `function` should (almost) always be followed by squiggly brackets (`{}`), and the contents should be indented by two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.

An opening curly brace should never go on its own line and should always be followed by a new line. A closing curly brace should always go on its own line, unless it's followed by `else`. Always indent the code inside curly braces.

```{r 19.4.3-1}
# Good
# if (y < 0 && debug) {
#   message("Y is negative")
# }
# 
# if (y == 0) {
#   log(x)
# } else {
#   y ^ x
# }
# 
# # Bad
# if (y < 0 && debug)
# message("Y is negative")
# 
# if (y == 0) {
#   log(x)
# } 
# else {
#   y ^ x
# }
```

It’s ok to drop the curly braces if you have a very short if statement that can fit on one line: 
```{r 19.4.3-2}
y <- 10
x <- if (y < 20) "Too low" else "Too high"
```

I recommend this only for very brief `if` statements. Otherwise, the full form is easier to read:
```{r}
if (y < 20) {
  x <- "Too low" 
} else {
  x <- "Too high"
}
```

## 19.5 Function arguments 
The arguments to a function typically fall into two broad sets: one set supplies the data to compute on, and the other supplies arguments that control the details of the computation. For example:

- In `log()`, the data is `x`, and the detail is the base of the `logarithm`.
- In `mean()`, the data is `x`, and the details are how much data to trim from the ends (`trim`) and how to handle missing values (`na.rm`).
- In `t.test()`, the data are `x` and `y`, and the details of the test are alternative, mu, paired, var.equal, and conf.level.
- In `str_c()` you can supply any number of strings to ..., and the details of the concatenation are controlled by `sep` and `collapse`.

Generally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument:
```{r}
# Compute confidence interval around mean using normal approximation
mean_ci <- function(x, conf = 0.95) {
  se <- sd(x) / sqrt(length(x))
  alpha <- 1 - conf
  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))
}

x <- runif(100)
mean_ci(x)
#> [1] 0.498 0.610
mean_ci(x, conf = 0.99)
#> [1] 0.480 0.628
```

The default value should almost always be the most common value. The few exceptions to this rule are to do with safety. For example, it makes sense for `na.rm` to default to `FALSE` because missing values are important. Even though `na.rm = TRUE` is what you usually put in your code, it’s a bad idea to silently ignore missing values by default.

When you call a function, you typically omit the names of the data arguments, because they are used so commonly. If you override the default value of a detail argument, you should use the full name:
```{r}
# Good
mean(1:10, na.rm = TRUE)

# Bad
mean(x = 1:10, , FALSE)
mean(, TRUE, x = c(1:10, NA))
```

You can refer to an argument by its unique prefix (e.g. `mean(x, n = TRUE)`), but this is generally best avoided given the possibilities for confusion.

Notice that when you call a function, you should place a space around `=` in function calls, and always put a space after a comma, not before (just like in regular English). Using whitespace makes it easier to skim the function for the important components.
```{r}
# Good
# average <- mean(feet / 12 + inches, na.rm = TRUE)

# Bad
# average<-mean(feet/12+inches,na.rm=TRUE)
```

### 19.5.1 Choosing names
The names of the arguments are also important. R doesn’t care, but the readers of your code (including future-you!) will. Generally you should prefer longer, more descriptive names, but there are a handful of very common, very short names. It’s worth memorising these:

- x, y, z: vectors.
- w: a vector of weights.
- df: a data frame.
- i, j: numeric indices (typically rows and columns).
- n: length, or number of rows.
- p: number of columns.

Otherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed.

### 19.5.2 Cheking values
As you start to write more functions, you’ll eventually get to the point where you don’t remember exactly how your function works. At this point it’s easy to call your function with invalid inputs. To avoid this problem, it’s often useful to make constraints explicit. For example, imagine you’ve written some functions for computing weighted summary statistics:
```{r}
wt_mean <- function(x, w) {
  sum(x * w) / sum(w)
}
wt_var <- function(x, w) {
  mu <- wt_mean(x, w)
  sum(w * (x - mu) ^ 2) / sum(w)
}
wt_sd <- function(x, w) {
  sqrt(wt_var(x, w))
}
```

What happens if x and w are not the same length?
```{r}
wt_mean(1:6, 1:3)
#> [1] 7.67
```

In this case, because of R’s vector recycling rules, we don’t get an error.

It’s good practice to check important preconditions, and throw an error (with `stop()`), if they are not true:
```{r}
wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  sum(w * x) / sum(w)
}
```

Be careful not to take this too far. There’s a tradeoff between how much time you spend making your function robust, versus how long you spend writing it. For example, if you also added a `na.rm` argument, I probably wouldn’t check it carefully:
```{r}
wt_mean <- function(x, w, na.rm = FALSE) {
  if (!is.logical(na.rm)) {
    stop("`na.rm` must be logical")
  }
  if (length(na.rm) != 1) {
    stop("`na.rm` must be length 1")
  }
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
```

This is a lot of extra work for little additional gain. A useful compromise is the built-in `stopifnot()`: it checks that each argument is `TRUE`, and produces a generic error message if not.
```{r}
wt_mean <- function(x, w, na.rm = FALSE) {
  stopifnot(is.logical(na.rm), length(na.rm) == 1)
  stopifnot(length(x) == length(w))
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
# wt_mean(1:6, 6:1, na.rm = "foo")
#> Error in wt_mean(1:6, 6:1, na.rm = "foo"): is.logical(na.rm) is not TRUE
```

Note that when using stopifnot() you assert what should be true rather than checking for what might be wrong.

### 19.5.3 Dot-dot-dot(...)
Many functions in R take an arbitrary number of inputs: 
```{r 19.5.3-1}
sum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
#> [1] 55
stringr::str_c("a", "b", "c", "d", "e", "f")
#> [1] "abcdef"
```

How do these functions work? They rely on a special argument: `...` (pronounced dot-dot-dot). This special argument captures any number of arguments that aren’t otherwise matched.

It’s useful because you can then send those `...` on to another function. This is a useful catch-all if your function primarily wraps another function. For example, I commonly create these helper functions that wrap around `str_c()`:
```{r 19.5.3-2}
commas <- function(...) stringr::str_c(..., collapse = ", ")
commas(letters[1:10])
#> [1] "a, b, c, d, e, f, g, h, i, j"

rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", stringr::str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
#> Important output ------------------------------------------------------
```

Here ... lets me forward on any arguments that I don’t want to deal with to `str_c()`. It’s a very convenient technique. But it does come at a price: any misspelled arguments will not raise an error. This makes it easy for typos to go unnoticed:
```{r 19.5.3-3}
x <- c(1,2)
sum(x,na.rm=T)
```

If you just want to capture the values of the `...`, use `list(...)`.

### 19.5.4 Lazy evaluation
Arguments in R are lazily evaluated: they’re not computed until they’re needed. That means if they’re never used, they’re never called. This is an important property of R as a programming language, but is generally not important when you’re writing your own functions for data analysis. You can read more about lazy evaluation at http://adv-r.had.co.nz/Functions.html#lazy-evaluation.

### 19.5.5 Exercises
1. What does commas(letters, collapse = "-") do? Why?
2. It’d be nice if you could supply multiple characters to the pad argument, e.g. rule("Title", pad = "-+"). Why doesn’t this currently work? How could you fix it?
3. What does the trim argument to mean() do? When might you use it?
4. The default value for the method argument to cor() is c("pearson", "kendall", "spearman"). What does that mean? What value is used by default?

## 19.6 Return values
Figuring outwhat your function should is usually straightfoward: it's why you created the function in the first place! There are two things you should consider when returning a value: 
1. Does returning early make your function easier to read? 
2. Can you make your function pipeable?

### 19.6.1 Explicit return statements 
The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using `return()`. I think it’s best to save the use of `return()` to signal that you can return early with a simpler solution. A common reason to do this is because the inputs are empty:
```{r 19.6.1-1}
complicated_function <- function(x,y,z){
  if (lenth(x)==0 || length(y)==0){
    return(0)
  }
  # completated code is here
}
```

Another reason is because you have a `if` statement with one complex block and simple block. For example, you might write if statement like this:
```{r 19.6.1-2}
f <- function() {
  if (x) {
    # Do 
    # something
    # that
    # takes
    # many
    # lines
    # to
    # express
  } else {
    # return something short
  }
}
```

But if the first block is very long, by the time you get to the else, you’ve forgotten the condition. One way to rewrite it is to use an early return for the simple case:
```{r 19.6.1-3}
f <- function() {
  if (!x) {
    return(something_short)
  }

  # Do 
  # something
  # that
  # takes
  # many
  # lines
  # to
  # express
}
```

This tends to make the code easier to understand, because you don't need quite so much context to understand it.

### 19.6.2 Writing pipeable functions
If you want to write your own pipeable functions, it’s important to think about the return value. Knowing the return value’s object type will mean that your pipeline will “just work”. For example, with dplyr and tidyr the object type is the data frame.

There are two basic types of pipeable functions: transformations and side-effects. With transformations, an object is passed to the function’s first argument and a modified object is returned. With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline. For example, this simple function prints the number of missing values in a data frame:

```{r 19.6.2-1}
show_missing <- function(df){
  n <- sum(is.na(df))
  cat("Missing values:",n,"\n",sep="")
  
  invisible(df)
}
```

If we call it interatively, the `invisible()` means that the input `df` does not get printed out: 
```{r 19.6.2-2}
show_missing(mtcars)
```

But it's still there, it's not just printed by default:
```{r 19.6.2-3}
x <- show_missing(mtcars)
class(x)
dim(x)
```

And we can still use it in a pipe:
```{r 19.6.2-4}
library(magrittr)
library(tidyverse)

mtcars %>% 
  show_missing() %>% 
  mutate(mpg=ifelse(mpg<20,NA,mpg)) %>% 
  show_missing()
```

## 19.7 Environment
The last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work. The environment of a function controls how R finds the value associated with a name. For example, take this function:
```{r 19.7-1}
f <- function(x){
  x+y
}
```

In many programming languages, this would be an error, because `y` is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since `y` is not defined inside the function, R will look in the environment where the function was defined:
```{r 19.7-2}
y <- 100
f(10)
y <- 1000
f(10)
```

This behavious seems like a recipe for bugs, and indeed you should avoid creating functions like this deliberately, but by and large it doesn’t cause too many problems (especially if you regularly restart R to get to a clean slate).

The advantage of this behaviour is that from a language standpoint it allows R to be very consistent. Every name is looked up using the same set of rules. For `f()` that includes the behaviour of two things that you might not expect: `{` and `+`. This allows you to do devious things like:
```{r 19.7-3}
`+` <- function(x, y) {
  if (runif(1) < 0.1) {
    sum(x, y)
  } else {
    sum(x, y) * 1.1
  }
}
table(replicate(1000, 1 + 2))
#> 
#>   3 3.3 
#> 100 900
rm(`+`)
```

This is a common phenomenon in R. R places few limits on your power. You can do many things that you can’t do in other programming languages. You can do many things that 99% of the time are extremely ill-advised (like overriding how addition works!). But this power and flexibility is what makes tools like ggplot2 and dplyr possible. Learning how to make best use of this flexibility is beyond the scope of this book, but you can read about in Advanced R.

# Chapter 20: Vectors 
## 20.1 Introduction
So far this book has focussed on tibbles and packages that work with them. But as you start to write your own functions, and dig deeper into R, you need to learn about vectors, the objects that underlie tibbles. If you’ve learned R in a more traditional way, you’re probably already familiar with vectors, as most R resources start with vectors and work their way up to tibbles. I think it’s better to start with tibbles because they’re immediately useful, and then work your way down to the underlying components.

Vectors are particularly important as most of the functions you will write will work with vectors. It is possible to write functions that work with tibbles (like ggplot2, dplyr, and tidyr), but the tools you need to write such functions are currently idiosyncratic and immature. I am working on a better approach, https://github.com/hadley/lazyeval, but it will not be ready in time for the publication of the book. Even when complete, you’ll still need to understand vectors, it’ll just make it easier to write a user-friendly layer on top.

### 20.1.1 PRerequisites 
The focus of this chapter is on base R data structures, so it isn’t essential to load any packages. We will, however, use a handful of functions from the purrr package to avoid some inconsistencies in base R.
```{r 20.1.1}
library(tidyverse)
```

## 20.2 Vector basics 
There are two types of vectors: 
1. Atomic vectors, of which there are six types: logical, integer, double, character, complex, and raw. Integer and double vectors are collectively known as numeric vectors.
2. Lists, which are sometimes called recursive vectors because lists can contain other lists.

The chief difference between atomic vectors and lists is that atomic vectors are homogeneous, while lists can be heterogeneous. There’s one other related object: NULL. NULL is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0. Figure 20.1 summarises the interrelationships.

Every vector has two key properties:
1. Its type, which you can determine with `typeof()`.
```{r}
typeof(letters)
#> [1] "character"
typeof(1:10)
#> [1] "integer"
```

2. Its length, which you can determine with `length()`
```{r}
x <- list("a","b",1:10)
length(x)
x
```

Vectors can also contain arbitrary additional metadata in the form of attributes. These attributes are used to create augmented vectors which build on additional behaviour. There are three important types of augmented vector:
- Factors are built on top of integer vectors.
- Dates and date-times are built on top of numeric vectors.
- Data frames and tibbles are built on top of lists.

This chapter will introduce you to these important vectors from simplest to most complicated. You’ll start with atomic vectors, then build up to lists, and finish off with augmented vectors.

## 20.3 Important types of atomic vector
The four most important types of atomic vector are logical, integer, double, and character. Raw and complex are rarely used during a data analysis, so I won’t discuss them here.

### 20.3.1 Logical
Logical vectors are the simplest type of atomic vector because they can take only three possible values: `FALSE`, `TRUE`, and `NA`. Logical vectors are usually constructed with comparison operators, as described in comparisons. You can also create them by hand with `c()`:
```{r}
1:10 %% 3 ==0
c(T,T,F,NA)
```

### 20.3.2 Numeric
Integer and double vectors are known collectively as numeric vectors. In R, numbers are doubles by default. To make an integer, place an L after the number:
```{r 20.3.2-1}
typeof(1)
typeof(1L)
1.5
```

The distinction between integers and doubles is not usually important, but there are two important differences that you should be aware of:

1. Doubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations. For example, what is square of the square root of two?
```{r 20.3.2-2}
x <- sqrt(2)^2
x
x-2

#This behaviour is common when working with floating point numbers: most calculations include some approximation error. Instead of comparing floating point numbers using `==`, you should use `dplyr::near()` which allows for some numerical tolerance.
```

2. Integers have one special value: `NA`, while doubles have four `NA`,`NaN`,`Inf`,and `-Inf`. All these special values can arise during division:
```{r 20.3.2-3}
c(-1,0,1)%/% 0
# [1] -Inf  NaN  Inf
```

Avoid using == to check for these other special values. Instead use the helper functions `is.finite()`, `is.infinite()`, and `is.nan()`:

### 20.3.3 Character
Character vectors are the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data.

You’ve already learned a lot about working with strings in strings. Here I wanted to mention one important feature of the underlying string implementation: R uses a global string pool. This means that each unique string is only stored in memory once, and every use of the string points to that representation. This reduces the amount of memory needed by duplicated strings. You can see this behaviour in practice with `pryr::object_size()`:
```{r 20.3.3}
x <- "This is a reasonably long string."
pryr::object_size(x)

y <- rep(x,1000)
pryr::object_size(y)
```

y doesn’t take up 1,000x as much memory as x, because each element of y is just a pointer to that same string. A pointer is 8 bytes, so 1000 pointers to a 136 B string is 8 * 1000 + 136 = 8.13 kB.

### 20.3.4 Missing values
Note that each type of atomic vector has its own missing value:
```{r}
NA            # logical
#> [1] NA
NA_integer_   # integer
#> [1] NA
NA_real_      # double
#> [1] NA
NA_character_ # character
#> [1] NA
```

Normally you don’t need to know about these different types because you can always use `NA` and it will be converted to the correct type using the implicit coercion rules described next. However, there are some functions that are strict about their inputs, so it’s useful to have this knowledge sitting in your back pocket so you can be specific when needed.

### 20.3.5 Exercises
**1. Describe the difference between `is.finite(x)` and !is.infinite(x).**  
There is no difference! `is.finite()` tests whether a value is **not** `-Inf` or `Inf` and `is.infinite` tests whether it is `-Inf` or `Inf`. Logically, `!` negates the previous.

However... `is.infinite` `NA` and `NaN` to be non infinite! See the answer to ex 19.3.1 here.

**2. Read the source code for `dplyr::near()` (Hint: to see the source code, drop the ()). How does it work?**  
It calculates absolute difference up to a certain tolerance level. If the difference is 0.2 it calculates the difference and compares it to the tolerance level.

**3. A logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research.**
Taken from: https://jrnold.github.io/r4ds-exercise-solutions/vectors.html

**4. Brainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise.**
```{r}
x <- seq(-10,10,by=0.5)
as.integer(x) # turns to a literal integer
floor(x) # rounds down
`%/%`(x,1) # extracts the integer part of x
```

**5. What functions from the readr package allow you to turn a string into logical, integer, and double vector?**
```{r}
library(tidyverse)
x <- c("TRUE", "FALSE")
parse_logical(x)
x <- c("1", "2")
parse_integer(x)
x <- c("1.1", "1.2")
parse_double(x)
```

## 20.4 Using atomic vectors 
Now that you understand the different types of atomic vector, it’s useful to review some of the important tools for working with them. These include:

1. How to convert from one type to another, and when that happens automatically.
2. How to tell if an object is a specific type of vector.
3. What happens when you work with vectors of different lengths.
4. How to name the elements of a vector.
5. How to pull out elements of interest.

### 20.4.1 Coercion
There are two ways to convert or coerce, one type of vector to another: 

1. Explicit coercion happens when you call a function like `as.logical()`, `as.integer()`, `as.double()`, or `as.character()`. Whenever you find yourself using explicit coercion, you should always check whether you can make the fix upstream, so that the vector never had the wrong type in the first place. For example, you may need to tweak your readr col_types specification.
2. Implicit coercion happens when you use a vector in a specific context that expects a certain type of vector. For example, when you use a logical vector with a numeric summary function, or when you use a double vector where an integer vector is expected.

Because explicit coercion is used relatively rarely, and is largely easy to understand, I’ll focus on implicit coercion here.

You’ve already seen the most important type of implicit coercion: using a logical vector in a numeric context. In this case `TRUE` is converted to `1` and `FALSE` converted to `0`. That means the sum of a logical vector is the number of trues, and the mean of a logical vector is the proportion of trues:
```{r 20.4.1-1}
x <- sample(20,100,replace=T)
y <- x > 10
sum(y) # how many are greater than 10?
mean(y) # what proportion are greater than 10?
```

You may see some code (typically older) that relies on implicit coercion in the opposite direction, from integer to logical:
```{r 20.4.1-2}
if (length(x)){
  # do something
}
```

In this case, 0 is converted to `FALSE` and everything else is converted to `TRUE`. I think this makes it harder to understand your code, and I don’t recommend it. Instead be explicit: `length(x) > 0`.

It’s also important to understand what happens when you try and create a vector containing multiple types with `c()`: the most complex type always wins.
```{r 20.4.1-3}
typeof(c(TRUE,1L))
typeof(c(1L,1.5))
typeof(c(1.5,"a"))
```

An atomic vector can not have a mix of different types because the type is a property of the complete vector, not the individual elements. If you need to mix multiple types in the same vector, you should use a list, which you’ll learn about shortly.

### 20.4.2 Test functions
Sometimes you want to do different things based on the type of vector. One option is to use `typeof()`. Another is to use a test function which returns a `TRUE` or `FALSE`. Base R provides many functions line `is.vector()` and `is.atomic()`, but they often returns surprising results. Instead, it's safer to use the `is_*` functions provided by purrr, which are summarised in the table below. 

Each predicate also comes with a “scalar” version, like `is_scalar_atomic()`, which checks that the length is 1. This is useful, for example, if you want to check that an argument to your function is a single logical value.

### 20.4.3 Scalars and recicling rules
As well as implicitly coercing the types of vectors to be compatible, R will also implicitly coerce the length of vectors. This is called vector **recycling**, because the shorter vector is repeated, or recycled, to the same length as the longer vector.

This is generally most useful when you are mixing vectors and “scalars”. I put scalars in quotes because R doesn’t actually have scalars: instead, a single number is a vector of length 1. Because there are no scalars, most built-in functions are vectorised, meaning that they will operate on a vector of numbers. That’s why, for example, this code works:
```{r}
sample(10)+100
runif(10)>0.5
```

In R, basic mathematical operations work with vectors. That means that you should never need to perform explicit iteration when performing simple mathematical computations.

It’s intuitive what should happen if you add two vectors of the same length, or a vector and a “scalar”, but what happens if you add two vectors of different lengths?
```{r}
1:10 +1:2
```

Here, R will expand the shortest vector to the same length as the longest, so called recycling. This is silent except when the length of the longer is not an integer multiple of the length of the shorter:
```{r}
1:10+1:3
```

While vector recycling can be used to create very succinct, clever code, it can also silently conceal problems. For this reason, the vectorised functions in tidyverse will throw errors when you recycle anything other than a scalar. If you do want to recycle, you’ll need to do it yourself with `rep()`:
```{r}
library(tidyverse)

# tibble(
#   x=1:4,
#   y=1:2
# )

tibble(
  x=1:4,
  y=rep(1:2,each=2)
)
```

#### 20.4.4 Naming vectors
All types of vectors can be named. You can name them during creatin with `c()`:
```{r 20.4.4-1}
c(x=1,y=2,z=4)
```

Or after the fact with `purr::set_names()`
```{r 20.4.4-2}
set_names(1:3,c("a","b","c"))
```

Named vectors are most useful for subsetting, described next.

### 20.4.5 Subsetting
So far, we have used `dplyr::filter` to filter the rows in a tibble. `filter()` only works with tibble, so we will need new tool for vectors: `[`. `[` is the subsetting function, and is called like `x[a]`. There are four types of things you can usbset a vector with: 

1. A numeric vector containing only integers. The integers must either be all positive, all negative, or zero. Subsetting with positive integers keeps the elements at those positions:
```{r}
x <- c("one","two","three","four","five")
x[c(3,2,5)]
```

By repeating a position, you can actually make a longer output than input:
```{r}
x[c(1,1,5,5,5,2)]
```

Negative values drop the elements at the specified positions:
```{r}
x[c(-1,-3,-5)]
```

It's an error to mix positive and negative values:
```{r}
# x[c(1,-1)]
```

The error message mentions subsetting with zero, which returns no values:
```{r}
x[0]
```

This is not useful very often, but it can be helpful if you want to create unusual data structures to test your functions with.

2. Subsetting with a logical vector keeps all values corresponding to a `TRUE` value. This is most often useful in conjunction with the comparison functions.
```{r}
library(tidyverse)
x <- c(10,3,NA,5,8,1)

# tibble test
x <- as.tibble(x,ncol=1)
names(x)="v1"
is.na(x)
x %>% filter(v1 == NA)

# all non-missing values of x
x <- c(10,3,NA,5,8,1)
x[!is.na(x)]

# all even (or missing) values of x
x[x %% 2==0]
```

3. If you have a named vector, you can subset it with a character vector:
```{r}
x <- c(abc=1, def=2,xyz=5)
x[c("xyz","def")]
```

Like with positive integers, you can use a character vector to duplicate individual entries.

4. The simplest type of subsetting is nothing, `x[]`, which returns the complete `x`. This is not useful for subsetting vectors, but it is useful when subsetting matrices (and other high dimensional) structures because it lets you select the rows or  all the columns, by leaving that index blank. For example, if x is `2d`, `x[1, ]` selects the first row and all the columns, and `x[, -1]` selects all rows and all columns except the first.

To learn more about the applications of subsetting, reading the “Subsetting” chapter of Advanced R: http://adv-r.had.co.nz/Subsetting.html#applications.

There is an important variation of [ called [[. [[ only ever extracts a single element, and always drops names. It’s a good idea to use it whenever you want to make it clear that you’re extracting a single item, as in a for loop. The distinction between [ and [[ is most important for lists, as we’ll see shortly.

### 20.4.6 Exercises
**1. What does `mean(is.na(x))` tell you about a vector x? What about `sum(!is.finite(x))`?**

`mean(is.na(x))` gives the proportion or percentage of missing values in that vector `sum(!is.finite(x))` gives the total number of infinite objects in x

**2. Carefully read the documentation of `is.vector()`. What does it actually test for? Why does `is.atomic()` not agree with the definition of atomic vectors above?**

`s.atomic()` tests for objects that area "logical", "integer", "numeric" (synonym "double"), "complex", "character" and "raw". `is.vector()` on the other hand tests for any of the atomic modes but must have no attributes other than names.

A factor, for example, shows how they contradict.
```{r}
x <- factor()
is.vector(x)
is.atomic(x)
```

**3. Compare and contrast `setNames()` with `purrr::set_names()`.**

The do the same thing but `purrr::set_names` but with stricter argument checking and cool argument to rename variable based on a function.

**4. Create functions that take a vector as input and returns:**

- The last value. Should you use `[` or `[[`?

`[[` because it want the specific values. `x` might have other attributes such as names and it is asking for one value.
```{r}
returner <- function(x){
  x[[length(x)]]
}
returner(1:10)
```

- The elements at even numbered positions.
```{r}
even_num <- function(x){
  x[seq(2,length(x),by=2)]
}
even_num(1:10)
```

- Every element except the last value.
```{r}
except_last <- function(x){
  x[-length(x)]
}
except_last(1:10)
```

- Only even numbers (and no missing values).
```{r}
even_numbers <- function(x){
  x[!is.na(x) & x%%2==0]
}
even_numbers(sample(c(NA,1:100),99))
```

**5. Why is `x[-which(x > 0)]` not the same as `x[x <= 0]`?**

x[-which(x > 0)] subset the values of x which ar at 0 or below x[x <= 0] subset the values of x which ar at 0 or below.

They are the same.

However, which will ignore NA's and leave them as is and <= will turn any value that cannot be comparable to NA like NaN!
```{r}
x <- c(-5:5, Inf, -Inf, NaN, NA)

x[-which(x > 0)]
x[x <= 0]
```

**6. What happens when you subset with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist?**

```{r}
x <- 1:10
x[11]

# Returns an NA
x <- c("a" = 1)
x["b"]
# Both the name and element is NA.
```

## 20.5 Recursive vectors (lists)
Lists are a step up in complexity from atomic vectors, because lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures. You create a list with `list()`:
```{r 20.5-1}
x <- list(1,2,3)
x
```

A very useful tool for working with lists is `str()` because it focuses on the structure, not the contents. 
```{r 20.5-2}
str(x)
x_named <- list(a=1,b=2,c=3)
str(x_named)
```

Unlike atomic vectors, `list()` can contain a mix of objects:
```{r 20.5-3}
y <- list("a",1L,1.5,T)
str(y)
```

List can even contain other lists!
```{r}
z <- list(list(1,2),list(3,4))
str(z)
```

### 20.5.1 Visualizing lists 
To explain more complicated list manipulation functions, it’s helpful to have a visual representation of lists. For example, take these three lists:
```{r 20.5.1}
x1 <- list(c(1,2),c(3,4))
x2 <- list(list(1,2),list(3,4))
x3 <- list(1,list(2,list(3)))
x1
x2
x3
```

There are three principles:

1. Lists have rounded corners. Atomic vectors have square corners.
2. Children are drawn inside their parent, and have a slightly darker background to make it easier to see the hierarchy.
3. The orientation of the children (i.e. rows or columns) isn’t important, so I’ll pick a row or column orientation to either save space or illustrate an important property in the example.

### 20.5.2 Subsetting
There are three ways ti subset a list, which I'll illustrate with a list named `a`:
```{r}
a <- list(a = 1:3, b = "a string", c = pi, d = list(-1, -5))
```

- `[` extracts a sub-list. The result will always be a list.
```{r 20.5.2-2}
str(a)
str(a[1:2])
str(a[4])
```

Like with vectors, you can subset with a logical, integer, or character vector. 

- `[[` extracts a single component from a list. It removes a level of hierarchy from the list. 
```{r 20.5.2-3}
str(a[[1]])
str(a[[4]])
```

- `$` is a shorthand for extracting named elements of a list. It works similarly to [[ except that you don’t need to use quotes.
```{r}
a$a
a[["a"]]
```

The distinction between `[` and `[[` is really important for lists, because `[[` drills down into the list while [ returns a new, smaller list. Compare the code and output above with the visual representation in Figure 20.2.

### 20.5.3 Lists of condiments
The difference between `[` and `[[` is very important, but it's easy to get confused. To help you remeber, let me show you an unusual peppr shaker. 

### 20.5.4 Exercies
1. Draw the following lists as nested sets:
```{r}
# list(a, b, list(c, d), list(e, f))
# list(list(list(list(list(list(a))))))
```

2. What happens if you subset a tibble as if you’re subsetting a list? What are the key differences between a list and a tibble?
```{r}
x <- as_tibble(mtcars)
x[1] # gives back a tibble (same as lists)
x[[1]] # gives back a vector (same as lists)
# In fact, data frames and tibbles are lists!!
```

## 20.6 Attributes
Any vector can contain arbitrary additional metadata through its attributes. You can think of attributes as named list of vectors that can be attached to any object. You can get and set individual attribute values with `attr()` or see them all at once with `attributes()`.
```{r 20.6-1}
x <- 1:10
attr(x,"greeting")

attr(x,"greeting") <- "Hi!"
attr(x,"farewell") <- "Bye!"
attributes(x)
```

There are three very important attributes that are used to inplement fundamental parts of R: 
1. **Names** are used to name the elements of a vector.
2. **Dimensions** (dims, for short) make a vector behave like a matrix or array.
3. **Class** is used to implement the S3 object oriented system.

You’ve seen names above, and we won’t cover dimensions because we don’t use matrices in this book. It remains to describe the class, which controls how generic functions work. Generic functions are key to object oriented programming in R, because they make functions behave differently for different classes of input. A detailed discussion of object oriented programming is beyond the scope of this book, but you can read more about it in Advanced R at http://adv-r.had.co.nz/OO-essentials.html#s3.

Here is what a typical generic function looks like: 
```{r 20.6-2}
as.Date
```

The call to “UseMethod” means that this is a generic function, and it will call a specific method, a function, based on the class of the first argument. (All methods are functions; not all functions are methods). You can list all the methods for a generic with `methods()`:
```{r 20.6-3}
methods("as.Date")
```

For example, if x is a character vector, `as.Date()` will call `as.Date.character()`; if it’s a factor, it’ll call `as.Date.factor()`.

You can see the specific implementation of a method with `getS3method()`
```{r}
getS3method("as.Date","default")
getS3method("as.Date","numeric")
```

The most important S3 generic is `print()`:  it controls how the object is printed when you type its name at the console. Other important generics are the subsetting functions `[`, `[[`, and `$`.

## 20.7 Augumented vectors 
Atomic vectors and lists are the building blocks for other important vector types like factors and dates. I call these **augmented vectors**, because they are vectors with additional **attributes**, including class. Because augmented vectors have a class, they behave differently to the atomic vector on which they are built. In this book, we make use of four important augmented vectors:
- Factors
- Dates 
- Date-times
- Tibbles

These are described below. 

### 20.7.1 Factors 
Factors are designed to represent categorical data that can take a fixed set of possible values. Factors are built on top of integers, and have a levels attribute:
```{r}
x <- factor(c("ab","cd","ab"),levels=c("ab","cd","ed"))
typeof(x)
attributes(x)
```

### 20.7.2 Dates and date-times
Dates in R are numeric vectors that represent the number of days since 1 January 1970.
```{r 20.7.2-1}
x <- as.Date("1971-01-01")
unclass(x)

typeof(x)
attributes(x)
```

Date-times are numeric vectors with class `POSIXct` that represent the number of seconds since 1 January 1970. (In case you were wondering, “POSIXct” stands for “Portable Operating System Interface”, calendar time.)
```{r 20.7.2-2}
x <- lubridate::ymd_hm("1970-01-01 01:00")
unclass(x)

typeof(x)
attributes(x)
```

The `tzone` attribute is optional. It controls how the time is printed, not what absolute time it refers to.
```{r 20.7.2-3}
attr(x,"tzone") <- "US/Pacific"
x

attr(x,"tzone") <- "US/Eastern"
x
```

There is another type of date-times called POSIXIt. There are built on top of named lists:
```{r}
y <- as.POSIXlt(x)
typeof(y)
#> [1] "list"
attributes(y)
```

POSIXlts are rare inside the tidyverse. They do crop up in base R, because they are needed to extract specific components of a date, like the year or month. Since lubridate provides helpers for you to do this instead, you don’t need them. POSIXct’s are always easier to work with, so if you find you have a POSIXlt, you should always convert it to a regular data time `lubridate::as_date_time()`.

### 20.7.3 Tibbles
Tibbles are augmented lists: they have class “tbl_df” + “tbl” + “data.frame”, and `names` (column) and `row.names` attributes:
```{r 20.7.3-1}
tb <- tibble::tibble(x = 1:5, y = 5:1)
typeof(tb)
#> [1] "list"
attributes(tb)
#> $names
#> [1] "x" "y"
#> 
#> $row.names
#> [1] 1 2 3 4 5
#> 
#> $class
#> [1] "tbl_df"     "tbl"        "data.frame"
```

The difference between a tibble and a list is that all the elements of a data frame must be vectors with the same length. All functions that work with tibbles enforce this constraint.

Traditional data.frames have a very similar structure:
```{r 20.7.3-2}
df <- data.frame(x = 1:5, y = 5:1)
typeof(df)
#> [1] "list"
attributes(df)
#> $names
#> [1] "x" "y"
#> 
#> $class
#> [1] "data.frame"
#> 
#> $row.names
#> [1] 1 2 3 4 5
```

The main difference is the class. The class of tibble includes “data.frame” which means tibbles inherit the regular data frame behaviour by default.

### 20.7.4 Exercises
1. What does hms::hms(3600) return? How does it print? What primitive type is the augmented vector built on top of? What attributes does it use?
```{r}
x <- hms::hms(3600)

class(x) # augmented classes
typeof(x)# a double, in fact, it is 3600 seconds if we use as.numeric
```

2. Try and make a tibble that has columns with different lengths. What happens?
```{r}
# The length of all columns must be the same or 1 (scalar), otherwise throws an error.
# tibble::tibble(a=1:2,b=1:3)
```

3. Based on the definition above, is it ok to have a list as a column of a tibble?
Yes, as long as it has the same length as other columns. But beware, because to 'open' that column all slots need to have the same class.

# Chapter 21: Iteration
## 21.1 Introduction 
In functions, we talked about how important it is to reduce duplication in your code by creating functions instead of copying-and-pasting. Reducing code duplication has three main benefits:

1. It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same.
2. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code.
3. You’re likely to have fewer bugs because each line of code is used in more places.

One tool for reducing duplication is functions, which reduce duplication by identifying repeated patterns of code and extract them out into independent pieces that can be easily reused and updated. Another tool for reducing duplication is iteration, which helps you when you need to do the same thing to multiple inputs: repeating the same operation on different columns, or on different datasets. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors.

### 21.1.1 Prerequisites
Once you’ve mastered the for loops provided by base R, you’ll learn some of the powerful programming tools provided by purrr, one of the tidyverse core packages.
```{r}
library(tidyverse)
```

## 21.2 For loops
Imagine we have this simple tibble:
```{r}
df <- tibble(
  a=rnorm(10),
  b=rnorm(10),
  c=rnorm(10),
  d=rnorm(10)
)
```

We want to compute the median of each column.
You could do with copy-and-paste.
```{r}
median(df$a)
median(df$b)
median(df$c)
median(df$d)
```

But that breaks our rule of thumb: never copy and paste more than twice. Instead, we could use a for loop:
```{r}
df
output <- vector("double",ncol(df))
for (i in seq_along(df)){
  output[[i]] <- median(df[[i]])
}
output <- tibble(output)
```

Every for loop has three components 
1. The `output: output <- vector("double", length(x))`. Before you start the loop, you must always allocate sufficient space for the output. This is very important for efficiency: if you grow the for loop at each iteration using `c()` (for example), your for loop will be very slow.

A general way of creating an empty vector of given length is the `vector()` function. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc) and the length of the vector.

2. The sequence: `i in seq_along(df)`. This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”.

You might not have seen `seq_along()` before. It’s a safe version of the familiar `1:length(l)`, with an important difference: if you have a zero-length vector, `seq_along()` does the right thing:
```{r}
y <- vector("double", 0)
seq_along(y)
#> integer(0)
1:length(y)
#> [1] 1 0
```

You probably won’t create a zero-length vector deliberately, but it’s easy to create them accidentally. If you use`1:length(x)` instead of `seq_along(x)`, you’re likely to get a confusing error message.

3. The body: `output[[i]] <- median(df[[i]])`. This is the code that does the work. It’s run repeatedly, each time with a different value for `i`. The first iteration will run `output[[1]] <- median(df[[1]])`, the second will run `output[[2]] <- median(df[[2]])`, and so on.

That’s all there is to the for loop! Now is a good time to practice creating some basic (and not so basic) for loops using the exercises below. Then we’ll move on some variations of the for loop that help you solve other problems that will crop up in practice.

### 21.2.1 Exercises
1. Write for loops to:
- Compute the mean of every column in mtcars.
- Determine the type of each column in `nycflights13::flights`.
- Compute the number of unique values in each column of `iris`.
- Generate 10 random normals for each of $\mu = -10,0,10 and 100$.
Think about the output, sequence, and body before you start writing the loop.
```{r}
looper <- function(the_type,object,fun){
  empty_vec=vector(the_type,length(object))
  
  for(every_column in seq_along(object)){
    empty_vec[every_column] <- fun(object[[every_column]])
  }
  empty_vec
}

# mtcars case
looper("numeric",mtcars,mean)
```

```{r}
# number of unique values iris
iris
looper("numeric",iris,function(x) sum(table(unique(x))))
```

```{r}
# generate 10 random normals for each of u =-10,0,10 and 100
ten_draws <- function(x){rnorm(10,mean=x)}
# map()
map(c(-10,0,10,100),ten_draws)
```

2. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:
```{r}
# out <- ""
# for (x in letters)`{}
```

3. Combine your function writing and for loop skills:

- Write a for loop that `prints()` the lyrics to the children’s song “Alice the camel”.
- Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure.
- Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface.
```{r}

```

4. It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step:

## 21.3 For loop variations
Once you have the basic for loop under your belt, there are some variations that you should be aware of. These variations are important regardless of how you do iteration, so don’t forget about them once you’ve mastered the FP techniques you’ll learn about in the next section.

There are four variations on the basic theme of the for loop:

1. Modifying an existing object, instead of creating a new object.
2. Looping over names or values, instead of indices.
3. Handling outputs of unknown length.
4. Handling sequences of unknown length.

### 21.3.1v Modifying an existing object
Sometimes, you want to use a for loop to modify an existing object. For example, remember our challenges from functions. We wanted to rescale every column in a data frame:
```{r}
library(tidyverse)

df <- tibble(
  a=rnorm(10),
  b=rnorm(10),
  c=rnorm(10),
  d=rnorm(10)
)

rescale01 <- function(x){
  rng <- range(x,na.rm=T)
  (x-rng[1])/(rng[2]-rng[1])
}

df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)

df
```

To sove this with a for loop, we again think about the three components:
1. Output: we already have the output — it’s the same as the input!
2. Sequence: we can think about a data frame as a list of columns, so we can iterate over each column with `seq_along(df)`.
3. Body: apply `rescale01()`.

This gives us:
```{r}
for ( i in seq_along(df)){
  df[[i]] <- rescale01(df[[i]])
}
```

### 21.3.2 Looping patterns
There are three basic ways to loop over a vector. So far I’ve shown you the most general: looping over the numeric indices with `for (i in seq_along(xs))`, and extracting the value with `x[[i]]`. There are two other forms:

1. Loop over the elements: `for (x in xs)`. This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently.

2. Loop over the names: `for (nm in names(xs))`. This gives you name, which you can use to access the value with `x[[nm]]`. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so:
```{r}
x
results <- vector("list",length(x))
names(results) <- names(x)
```

Iteration over the numeric indices is the most general form, because given the position you can extract both the name and the value: 
```{r}
for(i in seq_along(x)){
  name <- names(x)[[i]]
  value <- x[[i]]
}
```

### 21.3.3 Unknown output length
Sometimes you might not know how long the output will be. For example, imagine you want to simulate some random vectors of random lengths. You might be tempted to solve this problem by progressively growing the vector:
```{r}
means <- c(0,1,2)

output <- double()
for (i in seq_along(means)){
  n <- sample(100,1)
  output <- c(output,rnorm(n,means[[i]]))
}
str(output)
output
```

But this is not very efficient because in each iteration, R has to copy all the data from the previous iterations. In technical terms, you get "quadratic" $O(n^2)$ behaviour which means that a loop with three times as many elements would take nine $(3^2)$ times as long to run. 

A better solution to save the results in a list, and them combine into a single vector after the loop is done:
```{r}
out <- vector("list",length(means))
for (i in seq_along(means)){
  n <- sample(100,1)
  out[[i]] <- rnorm(n,means[[i]])
}
str(out)
str(unlist(out))
```

Here I’ve used `unlist()` to flatten a list of vectors into a single vector. A stricter option is to use `purrr::flatten_dbl()` — it will throw an error if the input isn’t a list of doubles.

This pattern occurs in other places too:
1. You might be generating a long string. Instead of `paste()`ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with `paste(output, collapse = "")`.

2. You might be generating a big data frame. Instead of sequentially `rbind()`ing in each iteration, save the output in a list, then use `dplyr::bind_rows(output)` to combine the output into a single data frame.

Watch out for this pattern. Whenever you see it, switch to a more complex result object, and then combine in one step at the end.

### 21.3.4 Unknown sequence length
Sometimes you don’t even know how long the input sequence should run for. This is common when doing simulations. For example, you might want to loop until you get three heads in a row. You can’t do that sort of iteration with the for loop. Instead, you can use a while loop. A while loop is simpler than for loop because it only has two components, a condition and a body:
```{r}
# while (condition){
#   #body
# }
```

A while loop is also more general than a for loop, because you can rewrite any for loop as a while loop, but you can't rewrite every while loop as for loop:
```{r}
# for (i in seq_along(x)){
#   #body
# }

# Equivalent to 
# i <- 1
# while(i <= length(x)){
#   #body
#   i <- i+1
# }

```

Herhow we could use a while loop to find how many tries it takes to get three heads in a row: 
```{r}
flip <- function() sample(c("T", "H"), 1)

flips <- 0
nheads <- 0

while (nheads < 3) {
  if (flip() == "H") {
    nheads <- nheads + 1
  } else {
    nheads <- 0
  }
  flips <- flips + 1
}
flips
#> [1] 3
```

I mention while loops only briefly, because I hardly ever use them. They’re most often used for simulation, which is outside the scope of this book. However, it is good to know they exist so that you’re prepared for problems where the number of iterations is not known in advance.

### 21.3.5 Exercises
1. Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, `files <- dir("data/", pattern = "\\.csv$", full.names = TRUE)`, and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame.
```{r}
all_csv <- c("one.csv","two.csv")
all_dfs <- vector("list",length(all_csv))

# for (i in all.csv){
#   all_dfs[[i]] <- read.csv(all_csv[[i]])
# }
# bind_rows(all_dfs)
```

2. What happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique?
```{r}
no_names <- 1:5
some_names <- c("one" = 1, 2, "three" = 3)
repeated_names <- c("one" = 1, "one" = 2, "three" = 3)

for (nm in names(no_names)) print(identity(nm)) # nothing happens!
for (nm in names(some_names)) print(identity(nm)) # the empty name get's filled with a ""
for (nm in names(repeated_names)) print(identity(nm)) # everything get's printed out
```

3. Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, `show_mean(iris)` would print:
```{r}
library(tidyverse)
# show_mean(iris)
#> Sepal.Length: 5.84
#> Sepal.Width:  3.06
#> Petal.Length: 3.76
#> Petal.Width:  1.20

show_means <- function(x) {
  
  the_class <- vector("logical", length(x))

  for (i in seq_along(x)) the_class[[i]] <- is.numeric(x[[i]])
  
  x <- x[the_class]
  
  for (i in seq_along(x)) {
    cat(paste0(names(x)[i], ": ", round(mean(x[[i]]), 2)), fill = TRUE)
  }
}

show_means(iris)
show_means(mtcars)
```

4. What does this code do? How does it work?
```{r}
trans <- list( 
  disp = function(x) x * 0.0163871,
  am = function(x) {
    factor(x, labels = c("auto", "manual"))
  }
)
for (var in names(trans)) {
  mtcars[[var]] <- trans[[var]](mtcars[[var]])
}
```
It converts `disp` and `am` by multiplying and then into a factor respectively. This is simply iterating over a list with functions, and applying in that same order to both variables.

## 21.4 For loops vs. functionals
For loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly.

To see why this is important, consider (again) this simple data frame:
```{r}
df <- tibble(
  a=rnorm(10),
  b=rnorm(10),
  c=rnorm(10),
  d=rnorm(10)
)
```

Imagine you want to compute the mean of every column. You could do that with a for loop:
```{r}
output <- vector("double",length(df))
for (i in seq_along(df)){
  output[[i]] <- mean(df[[i]])
}
output
```

You realize that you're going to want to compute the means of every column pretty frequently,so you extract it out into a function:
```{r}
col_mean <- function(df){
  output <- vector("double",length(df))
  for (i in seq_along(df)){
    output[i] <- mean(df[[i]])
  }
  output
}
```

But then you think it would also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your `col_mean()` function and replace the`mean()` with `median()` and `sd()`:
```{r}
col_median <- function(df){
  output <- vector("double",hh(df))
  for (i in seq_along(df)){
    output[i] <- median(df[[i]])
  }
  output
}

col_sd <- function(df){
  output <- vector("double",length(df))
  for (i in seq_along(df)){
    output[i] <- sd(df[[i]])
  }
  output
}

df
# col_median(df)
```

Uh oh! You’ve copied-and-pasted this code twice, so it’s time to think about how to generalise it. Notice that most of this code is for-loop boilerplate and it’s hard to see the one thing (`mean()`, `median()`, `sd()`) that is different between the functions.

What would you do if you saw a set of functions like this:
```{r}
f1 <- function(x) abs(x-mean(x))^1
f2 <- function(x) abs(x-mean(x))^2
f3 <- function(x) abs(x-mean(x))^3
```

Hopefully, you'd notice that there is a lot of duplication, and extract it out into an additional argument: 
```{r}
f <- function(x,i) abs(x-mean(x))^i
```

You have reduced the chance of bugs (because you know have 1/3 less code), and made it easy to generalize to new situations. 

We can do exactly the same thing with `col_mean()`, `col_median()` and `col_sd()` by adding an argument that supplies the function to apply to each column:
```{r}
col_summary <- function(df, fun) {
  out <- vector("double", length(df))
  for (i in seq_along(df)) {
    out[i] <- fun(df[[i]])
  }
  out
}
col_summary(df, median)
#> [1]  0.237 -0.218  0.254 -0.133
col_summary(df, mean)
#> [1]  0.2026 -0.2068  0.1275 -0.0917
```

The idea of passing a function  to another function is extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. It might take you a while to wrap your head around the idea, but it’s worth the investment. In the rest of the chapter, you’ll learn about and use the purrr package, which provides functions that eliminate the need for many common for loops. The apply family of functions in base R (`apply()`, `lapply()`, `tapply()`, etc) solve a similar problem, but purrr is more consistent and thus is easier to learn.

The goal of using purrr function instead of for loops is to allow you break common list manipulation challenges into independent pieces:

1. How can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list.

2. If you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe.

This structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code.

### 21.4.1 Exercises
1. Read the documentation for `apply()`. In the 2d case, what two for loops does it generalise?

2. Adapt `col_summary()` so that it only applies to numeric columns You might want to start with an `is_numeric()` function that returns a logical vector that has a TRUE corresponding to each numeric column.

## 21.5 The map functions
The pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output:

- `map()` makes a list.
- `map_lgl()` makes a logical vector.
- `map_int()` makes an integer vector.
- `map_dbl()` makes a double vector.
- `map_chr()` makes a character vector.

Each function takes a vector as input, applies a function to each piece, and then returns a new vector that's the same length(and has the same names) as the input. The type of the vector is determined by the suffix to the map function.

Once you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!).

Some people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years). The chief benefits of using functions like `map()` is not speed, but clarity: they make your code easier to write and to read.

We can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use `map_dbl()`:

```{r}
library(purrr)
head(df)


# Reference - for loop()
output <- vector("double",length(df))
for (i in seq_along(df)){
  output[[i]] <- mean(df[[i]])
}
output

map_dbl(df,mean)
map_dbl(df,median)
map_dbl(df,sd)
```

Compared to using a for loop, focus is on the operation being performed (i.e. `mean()`, `median()`, `sd()`), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe:
```{r}
df %>% map_dbl(mean)
df %>% map_dbl(median)
df %>% map_dbl(sd)
```

There are a few differences between `map_*()` and `col_summary()`:

- All purrr functions are implemented in C. This makes them a little faster at the expense of readability.
- The second argument, `.f`, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section.
- `map_*()` uses … ([dot dot dot]) to pass along additional arguments to `.f` each time it’s called:
```{r}
map_dbl(df,mean,trim=0.5)
```

- The map functions also preserve names:
```{r}
z <- list(x=1:3,y=4:5)
z

map_int(z,length)
```

### 21.5.1 Shortcuts 
There are a few shortcuts that you can use with `.f` in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits the up the `mtcars` dataset in to three pieces (one for each value of cylinder) and fits the same linear model to each piece:
```{r}
safe_log <- safely(log)
str(safe_log(10))
str(safe_log("a"))
```

When the functions suceeds, the result element contains the `result` and the error element is `NULL`. When the function fails, the `result` element is `NULL` and the `error` element contains an error object. 

`safely()` is designed to work with map:
```{r}
x <- list(1,10,"a")
y <- x %>% map(safely(log))
str(y)

```

This would be easier to work with if we had two lists: one of all the errors and one of all the output. That’s easy to get with `purrr::transpose()`:
```{r}
y <- x %>% transpose()
str(y)
```

It’s up to you how to deal with the errors, but typically you’ll either look at the values of `x` where `y` is an error, or work with the values of y that are ok:
```{r}
is_ok <- y$error %>% map_lgl(is_null)
x[!is_ok]
#> [[1]]
#> [1] "a"
# y$result[is_ok] %>% flatten_dbl()
#> [1] 0.0 2.3
```

Purrr provides two other useful adverbs:
1. Like `safely()`, `possibly()` always succeeds. It’s simpler than `safely()`, because you give it a default value to return when there is an error.
```{r}
x <- list(1,10,"a")
x %>% map_dbl(possibly(log,NA_real_))
```

2. `quietly()` performs a similar role to `safely()`, but instead of capturing errors, it captures printed output, messages, and warnings:
```{r}
x <- list(1,-1)
x %>% map(quietly(log)) %>% str()
```


## 21.7 Mapping over multiple arguments 
So far we’ve mapped along a single input. But often you have multiple related inputs that you need iterate along in parallel. That’s the job of the `map2()` and `pmap()` functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with `map()`:
```{r}
mu <- list(5,10,-3)
mu %>% 
  map(rnorm,n=5) %>% 
  str()
```

What if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds:
```{r}
sigma <- list(1,5,10)
seq_along(mu) %>% 
  map(~rnorm(5,mu[[.]],sigma[[.]])) %>% 
  str()
```

But that obfuscates the intent of the code. Instead we could use `map2()` which iterates over two vectors in parallel:
```{r}
map2(mu,sigma,rnorm,n=5) %>% str()
```

![map2 overview](C:/Users/kojikm.mizumura/Desktop/Data Science/R for Data Science/II. Wrangle/lists-map2.png)
Note that the arguments that vary for each call come before the function; arguments that are the same for every call come after.

Like `map()`, `map2()` is just a wrapper around a for loop:
```{r}
map2 <- function(x,y,f,...){
  out <- vector("list",length(x))
  for (i in seq_along(x)){
    out[[i]] <- f(x[[i]],y[[i]],...)
  }
  out
}
```

You could also imagine `map3()`, `map4()`, `map5()`, `map6()` etc, but that would get tedious quickly. Instead, purrr provides `pmap()` which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples:
```{r}

library(magrittr)
library(purrr)

n <- list(1,3,5)
args1 <- list(n,mu,sigma)
args1 %>% 
  pmap(rnorm) %>% 
  str()
```

f you don’t name the elements of list, `pmap()` will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments:

```{r}
args2 <- list(mean=mu, sd=sigma,n=n)
args2 %>% 
  pmap(rnorm) %>% 
  str()
```

That generates longer, but safer, calls:
Since the arguments are all the same length, it makes sense to store them in a data frame:
```{r}
library(tidyverse)
parms <- tribble(
  ~mean,~sd,~n,
  5,1,1,
  10,5,3,
  -3,10,5
)

parms %>% 
  pmap(rnorm)
```

As soon as your code gets complicated, I think a data frame is a good approach because it ensures that each column has a name and is the same length as all the other columns.

### 21.7.1 Involing different functions
There is one more step up in complexity - as well as varying the arguments to the function you might also vary the function itself:
```{r}
f <- c("runif","rnorm","rpois")
param <- list(
  list(min=-1,max=1),
  list(sd=5),
  list(lambda=10)
)

f
param
```

To handle this case, you can use `invoke_map()`:
```{r}
invoke_map(f,param,n=5) %>% 
  str()
```

The first argument is a list of functions or character vector of function names. The second argument is a list of lists giving the arguments that vary for each function. The subsequent arguments are passed on to every function.

And again, you can use `tribble()` to make creating these matching pairs a little easier:
```{r}
sim <- tribble(
  ~f,      ~params,
  "runif", list(min = -1, max = 1),
  "rnorm", list(sd = 5),
  "rpois", list(lambda = 10)
)
sim %>% 
  mutate(sim = invoke_map(f, params, n = 10))
```

## 21.8 Walk
Walk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk - the important thing is the action, not the return value. Here’s a very simple example:
```{r}
x <- list(1,"a",3)
x %>% 
  walk(print)
```

`walk()` is generally not that useful compared to `walk2()` or `pwalk()`. For example, if you had a list of plots and a vector of file names, you could use `pwalk()` to save each file to the corresponding location on disk:
```{r}
library(ggplot2)
plots <- mtcars %>% 
  split(.$cyl) %>% 
  map(~ggplot(., aes(mpg, wt)) + geom_point())
paths <- stringr::str_c(names(plots), ".pdf")

pwalk(list(paths, plots), ggsave, path = tempdir())
```

`walk()`, `walk2()` and `pwalk()` all invisibly return .x, the first argument. This makes them suitable for use in the middle of pipelines.

## 21.9 Other patterns of for loops
Purrr provides a number of other functions that abstract over other types of for loops. You’ll use them less frequently than the map functions, but they’re useful to know about. The goal here is to briefly illustrate each function, so hopefully it will come to mind if you see a similar problem in the future. Then you can go look up the documentation for more details.

### 21.9.1 PRedicate functions

A number of functions work with predicate functions that return either a single TRUE or FALSE.

`keep()` and `discard()` keep elements of the input where the predicate is TRUE or FALSE respectively:
```{r}
iris %>% 
  keep(is.factor) %>% 
  str()

iris %>% 
  discard(is.factor) %>%
  str()
```

`some()` and `every()` determine if the predicate is true for any or for all of the elements.
```{r}
x <- list(1:5,letters,list(10))

x %>% 
  some(is_character)

x %>% 
  detect(~.>5)

x %>% 
  detect_index(~.>5)
```

`head_while()` and `tail_while()` take elements from the start or end of a vector while a predicate is true:
```{r}
library(tidyverse)
library(magrittr)
x %>% 
  head_while(~.>5)
x %>% 
  tail_while(~.>5)
```

### 21.9.2 Reduce and accumulate
Sometimes you have a complex list that you want to reduce to a simple list by repeatedly applying a function that reduces a pair to a singleton. This is useful if you want to apply a two-table dplyr verb to multiple tables. For example, you might have a list of data frames, and you want to reduce to a single data frame by joining the elements together:
```{r}
dfs <- list(
  age=tibble(name="John",age=30),
  sex=tibble(name=c("John","Mary"),sex=c("M","F")),
  trt=tibble(name="Mary",treatment="A")
)

dfs %>% reduce(full_join)
```

Or maybe you have alist of vectors, and want to find the intersection:
```{r}
vs <- list(
  c(1,3,5,6,10),
  c(1,2,3,7,8,10),
  c(1,2,3,4,8,9,10)
)
vs %>% reduce(intersect)
```

The reduce function takes a "binary" function(i.e, a function with two primary inputs), and applies it repeatedly to a list until there is only a single element left.

Accumulate is imilar but it keeps all the interim results. You could use it to impletement a cumulative sum:
```{r}
x <- sample(10)
x
x %>% accumulate(`+`)
```




